{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GeForce RTX 2080 Ti'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "import inspect\n",
    "import time\n",
    "import logging\n",
    "import tmunlp as nlp\n",
    "import random\n",
    "\n",
    "from tqdm import trange, tqdm, tqdm_notebook, tqdm_pandas, tqdm_gui\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from transformers import BertConfig, BertModel, BertTokenizer, BertForSequenceClassification, AdamW, BertPreTrainedModel\n",
    "from transformers import get_constant_schedule_with_warmup\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "from torch import nn\n",
    "from torch.nn import MSELoss, CrossEntropyLoss\n",
    "\n",
    "FOLDER_PATH = './dataset/new/'\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load data & pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label dict:\n",
      "{'edu': 0, 'health': 1, 'politics': 2, 'sports': 3, 'tech': 4, 'travel': 5}\n",
      " Train data:\n",
      "   polarity                                           sentence\n",
      "0         1  半數癌症 疑是不良習慣導致 新頭殼newtalk 2011.12.07 徐千雅/綜合報導一項...\n",
      "1         3  MLB／葛蘭基若跳脫合約　身價可望再攀新高 記者吳婷雯／綜合報導今年季末花錢補強不手軟的洛杉...\n",
      "2         5  苑裡油菜花季 花田玩藝文 【聯合報╱記者祁容玉╱苑裡報導】 苑裡首度舉辦「稻苑裡賞花趣」油菜...\n",
      "3         4  營運不佳 諾基亞分割壓力大增 不到2個月前，諾基亞執行長埃洛普(Stephen Flop)在...\n",
      "4         5  苗栗╱拱天宮香客大樓 可遠眺好望角海岸 【聯合報╱記者祁容玉╱通霄報導】 頂樓可遠眺後龍好望...\n",
      "\n",
      " Test data:\n",
      "   polarity                                           sentence\n",
      "0         3  火箭遭蜂螫 苦吞6連敗 （路透加州聖克拉拉19日電）美國職籃NBA西區球隊休士頓火箭（Roc...\n",
      "1         4  奧運商機 中國電信推手機遊戲 （中央社記者鄭崇生上海26日電）倫敦奧運在大陸手機遊戲市場也有...\n",
      "2         4  超級月亮 台灣白天無緣見 當晚好天氣才有機會賞月〔自由時報記者林嘉琪／台北報導〕今年首波梅雨...\n",
      "3         5  網紋陶片出土 疑牛罵頭遺址   台中市日前在安和重劃區內，發現大批陶片，研判可能是牛罵頭文化...\n",
      "4         3  國民隊成敗 就看王建民肩膀 記者陳浚錡／綜合報導華盛頓國民隊今年先發投手陣容齊全，堪稱本季全...\n"
     ]
    }
   ],
   "source": [
    "def preprocessing(df):\n",
    "    df.polarity = [label_dict[i] for i in df.polarity]\n",
    "    return df\n",
    "\n",
    "def shuffle(df):\n",
    "    index = [i for i in range(df.shape[0])]\n",
    "    random.shuffle(index)\n",
    "    df = df.set_index([index]).sort_index()\n",
    "    return df\n",
    "\n",
    "\n",
    "train = pd.read_csv(FOLDER_PATH+'train.txt', sep='\\t', names=['polarity','sentence'])\n",
    "test = pd.read_csv(FOLDER_PATH+'test.txt', sep='\\t', names=['polarity','sentence'])\n",
    "\n",
    "train = shuffle(train)\n",
    "test = shuffle(test)\n",
    "\n",
    "label_list = sorted(list(set(train.polarity)))\n",
    "label_dict = {}\n",
    "for i in range(len(label_list)):\n",
    "    label_dict[label_list[i]] = i\n",
    "\n",
    "train = preprocessing(train)\n",
    "test = preprocessing(test)\n",
    "'''\n",
    "def combine(df):\n",
    "    df.sentence = [df.title[i]+' '+str(df.sentence[i]) for i in range(df.shape[0])]\n",
    "    return df\n",
    "\n",
    "train = pd.read_csv('./dataset/reader_emotion/train.txt', sep='\\t', names=['polarity', 'title', 'sentence'], header=None)\n",
    "#train = train.append([train]*2, ignore_index=True)\n",
    "test = pd.read_csv('./dataset/reader_emotion/test.txt', sep='\\t', names=['polarity', 'title', 'sentence'], header=None)\n",
    "\n",
    "train = shuffle(train)\n",
    "test = shuffle(test)\n",
    "\n",
    "label_list = sorted(list(set(train.polarity)))\n",
    "label_dict = {}\n",
    "for i in range(len(label_list)):\n",
    "    label_dict[label_list[i]] = i\n",
    "\n",
    "train = preprocessing(train)\n",
    "test = preprocessing(test)\n",
    "\n",
    "train = combine(train)\n",
    "test = combine(test)\n",
    "'''\n",
    "\n",
    "print('Label dict:\\n{}\\n Train data:\\n{}\\n\\n Test data:\\n{}'.format(label_dict, train.head(5), test.head(5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Build BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"directionality\": \"bidi\",\n",
       "  \"finetuning_task\": null,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"is_decoder\": false,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"num_labels\": 6,\n",
       "  \"output_attentions\": false,\n",
       "  \"output_hidden_states\": true,\n",
       "  \"output_past\": true,\n",
       "  \"pooler_fc_size\": 768,\n",
       "  \"pooler_num_attention_heads\": 12,\n",
       "  \"pooler_num_fc_layers\": 3,\n",
       "  \"pooler_size_per_head\": 128,\n",
       "  \"pooler_type\": \"first_token_transform\",\n",
       "  \"pruned_heads\": {},\n",
       "  \"torchscript\": false,\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_bfloat16\": false,\n",
       "  \"vocab_size\": 21128\n",
       "}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_LABELS = len(label_list)\n",
    "\n",
    "class BertForSequenceClassificationBonz(BertPreTrainedModel):\n",
    "   \n",
    "    def __init__(self, config):\n",
    "        super(BertForSequenceClassificationBonz, self).__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, self.config.num_labels)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        weights=None\n",
    "    ):\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds\n",
    "        )\n",
    "\n",
    "        pooled_output = outputs[1]\n",
    "\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n",
    "\n",
    "        if labels is not None:\n",
    "            if self.num_labels == 1:\n",
    "                #  We are doing regression\n",
    "                loss_fct = MSELoss()\n",
    "                loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "            else:\n",
    "                loss_fct = CrossEntropyLoss(weight=weights)\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            outputs = (loss,) + outputs\n",
    "\n",
    "        return outputs  # (loss), logits, (hidden_states), (attentions)\n",
    "\n",
    "class BertModelBonz():\n",
    "    def __init__(self, model='bert-base-chinese', batch_size=6, num_labels=2):\n",
    "        self.pre_trained_model = model\n",
    "        self.num_labels = num_labels\n",
    "        self.config = BertConfig.from_pretrained(self.pre_trained_model, output_hidden_states=True, num_labels=self.num_labels)\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(self.pre_trained_model)\n",
    "        \n",
    "        #self.model = BertForSequenceClassification(config=self.config)\n",
    "        self.model = BertForSequenceClassificationBonz.from_pretrained(self.pre_trained_model, config=self.config)\n",
    "        self.max_len = self.model.config.max_position_embeddings\n",
    "        self.optimizer = AdamW(params = self.model.parameters(), lr=1e-5)\n",
    "        \n",
    "        self.train_loss = []\n",
    "        self.train_accuracy = []\n",
    "        self.test_f1 = []\n",
    "        self.test_accuracy = []\n",
    "    \n",
    "    def create_ids(self, sentences):\n",
    "        logging.getLogger(\"transformers.tokenization_utils\").setLevel(logging.ERROR) #Disable tokenizer logs, it's really annoy\n",
    "        input_ids = []\n",
    "        for sen in tqdm(sentences, desc=\"Create Ids\"):\n",
    "            tmp = self.tokenizer.encode(sen)\n",
    "            input_ids.append(tmp)\n",
    "        input_ids = pad_sequences(input_ids, \n",
    "                                  maxlen=self.max_len, \n",
    "                                  dtype='int64', \n",
    "                                  truncating='post', \n",
    "                                  padding='post')\n",
    "        return input_ids\n",
    "    \n",
    "    def prepare_data(self, input_ids, input_labels=None):\n",
    "        input_ids = torch.tensor(self.create_ids(input_ids))\n",
    "        if input_labels is None:\n",
    "            return DataLoader(TensorDataset(input_ids), \n",
    "                              batch_size=self.batch_size,\n",
    "                              shuffle=True\n",
    "                             )\n",
    "        else:\n",
    "            input_labels = torch.tensor(input_labels)\n",
    "            return DataLoader(TensorDataset(input_ids, input_labels), \n",
    "                              batch_size=self.batch_size\n",
    "                             )\n",
    "    \n",
    "    def train(self, train_dataloader, test_dataloader=None, weights=None, epochs=4):\n",
    "        self.model.to(device)\n",
    "        for i in trange(epochs, desc=\"Epoch\"):\n",
    "            # Training model\n",
    "            self.model.train()\n",
    "            tr_loss = []\n",
    "            \n",
    "            for input_ids, input_labels in tqdm_notebook(train_dataloader, desc='Training'):\n",
    "                self.optimizer.zero_grad()\n",
    "                loss = self.model(input_ids=input_ids.cuda(), labels=input_labels.cuda(), weights=weights.cuda())[0] \n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                tr_loss.append(loss.item())\n",
    "            \n",
    "            loss_score = sum(tr_loss)/len(tr_loss)\n",
    "            self.train_loss.append(loss_score)\n",
    "\n",
    "            # Evaluation\n",
    "            self.model.eval()\n",
    "            predictions = []\n",
    "            labels = []\n",
    "            \n",
    "            for input_ids, input_labels in tqdm_notebook(train_dataloader, desc='Evaluating'):\n",
    "                with torch.no_grad():\n",
    "                    logits = self.model(input_ids=input_ids.cuda())[0]\n",
    "                logits = logits.detach().cpu().numpy()\n",
    "                predictions.extend(logits)\n",
    "                labels.extend(input_labels)\n",
    "            \n",
    "            predictions = np.argmax(predictions, axis=1)\n",
    "            acc_score = accuracy_score(labels, predictions)\n",
    "            self.train_accuracy.append(acc_score)\n",
    "            \n",
    "            # Print result\n",
    "            print('EPOCH', i)\n",
    "            print('Train loss: ', loss_score)\n",
    "            print('Train accuracy: ',acc_score)\n",
    "            print(classification_report(labels, predictions, digits=4))\n",
    "            \n",
    "            #Save model for each epoch:\n",
    "            filename = 'bert_512_epoch'+str(i)+'.sd'\n",
    "            filepath = FOLDER_PATH+filename\n",
    "            torch.save(self.model.state_dict(), filepath)\n",
    "            \n",
    "            if test_dataloader is not None:\n",
    "                # Predict test data\n",
    "                self.model.eval()\n",
    "                predictions = []\n",
    "\n",
    "                for input_ids, input_labels in tqdm_notebook(test_dataloader, desc=\"Predicting\"):\n",
    "                    with torch.no_grad():\n",
    "                        logits = self.model(input_ids=input_ids.cuda())[0] #This is for generate predict only\n",
    "                    logits = logits.detach().cpu().numpy()\n",
    "                    predictions.extend(logits)\n",
    "\n",
    "                predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "                self.test_f1.append(f1_score(test.polarity, predictions, average='macro'))\n",
    "                self.test_accuracy.append(accuracy_score(test.polarity, predictions))\n",
    "                \n",
    "                print(classification_report(test.polarity, predictions, digits=4))\n",
    "            \n",
    "    def generate_cls_vectors(self, dataloader):\n",
    "        self.model.to('cuda')\n",
    "        self.model.eval()\n",
    "        cls_vectors = []\n",
    "        for input_ids, input_labels in tqdm_notebook(dataloader):\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(input_ids.cuda())\n",
    "                last_hidden_layer = outputs[1][12]\n",
    "                cls_vector = last_hidden_layer[:,0,:]\n",
    "            cls_vector = cls_vector.detach().cpu().numpy()\n",
    "            cls_vectors.extend(cls_vector)\n",
    "        return cls_vectors\n",
    "                \n",
    "   \n",
    "        \n",
    "        \n",
    "#Create model\n",
    "bert_model = BertModelBonz(model='bert-base-chinese', batch_size=6, num_labels=NUM_LABELS)\n",
    "bert_model.model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Create train & test dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Create Ids: 100%|███████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 154.26it/s]\n",
      "Create Ids: 100%|███████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 240.64it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = bert_model.prepare_data(input_ids=train.sentence[:6], input_labels=train.polarity[:6])\n",
    "test_dataloader = bert_model.prepare_data(input_ids=test.sentence[:6], input_labels=test.polarity[:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Train model & Predict data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   0%|                                                                                     | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0e061455a084f5aa0dddff9bf5e16fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training', max=1, style=ProgressStyle(description_width='init…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of device type cuda but got device type cpu for argument #3 'weight' in call to _thnn_nll_loss_forward",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-06dcf4d85b24>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m                  \u001b[0mtest_dataloader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                  \u001b[0mweights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m                  epochs=4)\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-bb69e5188e61>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, train_dataloader, test_dataloader, weights, epochs)\u001b[0m\n\u001b[0;32m    107\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_labels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm_notebook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Training'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-bb69e5188e61>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, weights)\u001b[0m\n\u001b[0;32m     48\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m                 \u001b[0mloss_fct\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCrossEntropyLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    914\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    915\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[1;32m--> 916\u001b[1;33m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[0;32m    917\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   2007\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2008\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2009\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2010\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2011\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   1836\u001b[0m                          .format(input.size(0), target.size(0)))\n\u001b[0;32m   1837\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1838\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1839\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1840\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnll_loss2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected object of device type cuda but got device type cpu for argument #3 'weight' in call to _thnn_nll_loss_forward"
     ]
    }
   ],
   "source": [
    "#Train model\n",
    "\n",
    "bert_model.train(train_dataloader, \n",
    "                 test_dataloader,\n",
    "                 weights=torch.tensor([4,4,1,1,1,2], dtype=torch.float),\n",
    "                 epochs=4)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.8303304069182451, 1.5800249225953047, 1.4178658303092508, 1.1401410874198465, 0.32545239252850144, 0.21437513451076115, 0.1526120551652296, 0.11626514505056765] \n",
      " [0.39, 0.55, 0.76, 0.92, 0.9266406692664066, 0.9459554044595541, 0.96000399960004, 0.9724027597240276] \n",
      " [0.711570464874406] \n",
      " [0.7460433312845728]\n"
     ]
    }
   ],
   "source": [
    "print(bert_model.train_loss, '\\n', bert_model.train_accuracy, '\\n', bert_model.test_f1, '\\n', bert_model.test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Create Ids: 100%|███████████████████████████████████████████████████████████████| 78096/78096 [06:54<00:00, 198.50it/s]\n"
     ]
    }
   ],
   "source": [
    "test_dataloader = bert_model.prepare_data(input_ids=test.sentence, input_labels=test.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "027b14c41dbe4847905ad9e357ff2ab2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Predicting', max=13016, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.2989    0.8794    0.4461      5023\n",
      "           1     0.7195    0.9764    0.8285      5844\n",
      "           2     0.8041    0.8508    0.8268     19023\n",
      "           3     0.9270    0.8802    0.9030     18919\n",
      "           4     0.8774    0.6247    0.7298     17031\n",
      "           5     0.9019    0.3805    0.5352     12256\n",
      "\n",
      "   micro avg     0.7460    0.7460    0.7460     78096\n",
      "   macro avg     0.7548    0.7653    0.7116     78096\n",
      "weighted avg     0.8264    0.7460    0.7540     78096\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bert_model.model.eval()\n",
    "predictions = []\n",
    "\n",
    "for input_ids, input_labels in tqdm_notebook(test_dataloader, desc=\"Predicting\"):\n",
    "    with torch.no_grad():\n",
    "        logits = bert_model.model(input_ids=input_ids.cuda())[0] #This is for generate predict only\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    predictions.extend(logits)\n",
    "\n",
    "predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "bert_model.test_f1.append(f1_score(test.polarity, predictions, average='macro'))\n",
    "bert_model.test_accuracy.append(accuracy_score(test.polarity, predictions))\n",
    "\n",
    "print(classification_report(test.polarity, predictions, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Generate CLS vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2887f74e61d94e6284acbb89e3422fc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1946), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "091e7ace328e4e07ab629e2a4dec9d72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5934), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load state dict from trained model\n",
    "#filepath = './dataset/reader_emotion/bert512_epoch3.sd'\n",
    "#bert_model.model.load_state_dict(torch.load(filepath))\n",
    "\n",
    "cls_vectors = bert_model.generate_cls_vectors(train_dataloader)\n",
    "cls_vectors_test = bert_model.generate_cls_vectors(test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Feature Extraction with TMUNLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Build negative & positive vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "label_list = ['0', '1']\n",
    "\n",
    "result = nlp.get_label_term_weighting('./dataset/imdb/tmunlp_file.txt', label_list)\n",
    "VECTOR_LEN = 70\n",
    "\n",
    "#Create negative word list\n",
    "negative_list =  nlp.get_keyword('0', result, VECTOR_LEN)\n",
    "temp_max = negative_list[list(negative_list.keys())[0]]\n",
    "for i in negative_list.keys():\n",
    "    negative_list[i] = negative_list[i] #/ temp_max\n",
    "\n",
    "\n",
    "#Create postive word list\n",
    "positive_list =  nlp.get_keyword('1', result, VECTOR_LEN)\n",
    "temp_max = positive_list[list(positive_list.keys())[0]]\n",
    "for i in positive_list.keys():\n",
    "    positive_list[i] = positive_list[i] #/ temp_max\n",
    "\n",
    "\n",
    "\n",
    "def embedding_1hot(df, words):\n",
    "    arr = []\n",
    "    for i in range(df.shape[0]):\n",
    "        temp = [words[word] if word in df.sentence[i] else 0 for word in words.keys()]\n",
    "        arr.append(temp)\n",
    "    return arr\n",
    "\n",
    "train['negative_embedding'] = embedding_1hot(train, negative_list)\n",
    "train['positive_embedding'] = embedding_1hot(train, positive_list)\n",
    "\n",
    "test['negative_embedding'] = embedding_1hot(test, negative_list)\n",
    "test['positive_embedding'] = embedding_1hot(test, positive_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Build bert_llr model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertLLR(\n",
       "  (bert): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (bert_activation): Tanh()\n",
       "  (llr): Linear(in_features=140, out_features=140, bias=True)\n",
       "  (llr_activation): Tanh()\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=908, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BertLLR(nn.Module):\n",
    "    def __init__(self, VECTOR_LEN):\n",
    "        super(BertLLR, self).__init__()\n",
    "        self.bert = nn.Linear(768,768)\n",
    "        self.bert_activation = nn.Tanh()\n",
    "        \n",
    "        self.llr = nn.Linear(2*VECTOR_LEN, 2*VECTOR_LEN)\n",
    "        self.llr_activation = nn.Tanh()\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "        self.classifier = nn.Linear(768+2*VECTOR_LEN, 2)\n",
    "    \n",
    "    def forward(self, cls_vectors=None, neg_embed=None, pos_embed=None, labels=None):\n",
    "        tanh_cls_vectors = self.bert_activation(self.bert(cls_vectors))\n",
    "        \n",
    "        llr_vectors = torch.cat([neg_embed, pos_embed], dim=1)\n",
    "        tanh_llr_vectors = self.llr_activation(self.llr(llr_vectors))\n",
    "        \n",
    "        concat_vectors = torch.cat([tanh_cls_vectors, tanh_llr_vectors], dim=1)\n",
    "        concat_vectors = self.dropout(concat_vectors)\n",
    "        \n",
    "        logits = self.classifier(concat_vectors)\n",
    "        \n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, 2), labels.view(-1))\n",
    "            outputs = (logits, loss)\n",
    "        else:\n",
    "            outputs = (logits,)\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "bert_llr_model = BertLLR(VECTOR_LEN)\n",
    "bert_llr_optimizer = torch.optim.Adam(params = bert_llr_model.parameters(), \n",
    "                                      lr = 1e-5\n",
    "                                     )\n",
    "\n",
    "\n",
    "bert_llr_model   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Train bert_llr model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████| 1563/1563 [00:10<00:00, 146.91it/s]\n",
      "Predicting: 100%|█████████████████████████████████████████| 1563/1563 [00:01<00:00, 891.17it/s]\n",
      "Predicting: 100%|█████████████████████████████████████████| 1563/1563 [00:01<00:00, 809.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.03237039205795172\n",
      "Train accuracy:  0.99588\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9365    0.9309    0.9337     12500\n",
      "           1     0.9313    0.9369    0.9341     12500\n",
      "\n",
      "   micro avg     0.9339    0.9339    0.9339     25000\n",
      "   macro avg     0.9339    0.9339    0.9339     25000\n",
      "weighted avg     0.9339    0.9339    0.9339     25000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████| 1563/1563 [00:10<00:00, 148.19it/s]\n",
      "Predicting: 100%|█████████████████████████████████████████| 1563/1563 [00:01<00:00, 896.28it/s]\n",
      "Predicting: 100%|█████████████████████████████████████████| 1563/1563 [00:01<00:00, 913.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.015601529313760238\n",
      "Train accuracy:  0.99596\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9373    0.9307    0.9340     12500\n",
      "           1     0.9312    0.9378    0.9345     12500\n",
      "\n",
      "   micro avg     0.9342    0.9342    0.9342     25000\n",
      "   macro avg     0.9343    0.9342    0.9342     25000\n",
      "weighted avg     0.9343    0.9342    0.9342     25000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████| 1563/1563 [00:10<00:00, 148.45it/s]\n",
      "Predicting: 100%|█████████████████████████████████████████| 1563/1563 [00:01<00:00, 872.93it/s]\n",
      "Predicting: 100%|█████████████████████████████████████████| 1563/1563 [00:01<00:00, 992.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.014916918573570952\n",
      "Train accuracy:  0.99632\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9379    0.9304    0.9341     12500\n",
      "           1     0.9310    0.9384    0.9347     12500\n",
      "\n",
      "   micro avg     0.9344    0.9344    0.9344     25000\n",
      "   macro avg     0.9344    0.9344    0.9344     25000\n",
      "weighted avg     0.9344    0.9344    0.9344     25000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████| 1563/1563 [00:10<00:00, 146.02it/s]\n",
      "Predicting: 100%|█████████████████████████████████████████| 1563/1563 [00:01<00:00, 867.03it/s]\n",
      "Predicting: 100%|█████████████████████████████████████████| 1563/1563 [00:01<00:00, 875.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.014337929889264201\n",
      "Train accuracy:  0.99668\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9385    0.9307    0.9346     12500\n",
      "           1     0.9313    0.9390    0.9351     12500\n",
      "\n",
      "   micro avg     0.9348    0.9348    0.9348     25000\n",
      "   macro avg     0.9349    0.9348    0.9348     25000\n",
      "weighted avg     0.9349    0.9348    0.9348     25000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████| 1563/1563 [00:10<00:00, 150.83it/s]\n",
      "Predicting: 100%|█████████████████████████████████████████| 1563/1563 [00:01<00:00, 977.18it/s]\n",
      "Predicting: 100%|█████████████████████████████████████████| 1563/1563 [00:01<00:00, 945.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.01392129101979374\n",
      "Train accuracy:  0.99676\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9387    0.9308    0.9347     12500\n",
      "           1     0.9314    0.9392    0.9353     12500\n",
      "\n",
      "   micro avg     0.9350    0.9350    0.9350     25000\n",
      "   macro avg     0.9350    0.9350    0.9350     25000\n",
      "weighted avg     0.9350    0.9350    0.9350     25000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████| 1563/1563 [00:11<00:00, 136.84it/s]\n",
      "Predicting: 100%|█████████████████████████████████████████| 1563/1563 [00:01<00:00, 821.69it/s]\n",
      "Predicting: 100%|█████████████████████████████████████████| 1563/1563 [00:01<00:00, 913.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.013439818499317859\n",
      "Train accuracy:  0.9968\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9392    0.9306    0.9349     12500\n",
      "           1     0.9313    0.9398    0.9355     12500\n",
      "\n",
      "   micro avg     0.9352    0.9352    0.9352     25000\n",
      "   macro avg     0.9352    0.9352    0.9352     25000\n",
      "weighted avg     0.9352    0.9352    0.9352     25000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████| 1563/1563 [00:10<00:00, 150.01it/s]\n",
      "Predicting: 100%|█████████████████████████████████████████| 1563/1563 [00:01<00:00, 968.46it/s]\n",
      "Predicting: 100%|████████████████████████████████████████| 1563/1563 [00:01<00:00, 1008.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.013197794522653957\n",
      "Train accuracy:  0.99696\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9390    0.9309    0.9349     12500\n",
      "           1     0.9315    0.9395    0.9355     12500\n",
      "\n",
      "   micro avg     0.9352    0.9352    0.9352     25000\n",
      "   macro avg     0.9352    0.9352    0.9352     25000\n",
      "weighted avg     0.9352    0.9352    0.9352     25000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████| 1563/1563 [00:11<00:00, 136.65it/s]\n",
      "Predicting: 100%|█████████████████████████████████████████| 1563/1563 [00:01<00:00, 824.12it/s]\n",
      "Predicting: 100%|█████████████████████████████████████████| 1563/1563 [00:01<00:00, 891.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.012920671569866319\n",
      "Train accuracy:  0.99704\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9391    0.9307    0.9349     12500\n",
      "           1     0.9313    0.9396    0.9354     12500\n",
      "\n",
      "   micro avg     0.9352    0.9352    0.9352     25000\n",
      "   macro avg     0.9352    0.9352    0.9352     25000\n",
      "weighted avg     0.9352    0.9352    0.9352     25000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████| 1563/1563 [00:10<00:00, 151.77it/s]\n",
      "Predicting: 100%|█████████████████████████████████████████| 1563/1563 [00:01<00:00, 867.76it/s]\n",
      "Predicting: 100%|█████████████████████████████████████████| 1563/1563 [00:01<00:00, 904.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.012620894917628358\n",
      "Train accuracy:  0.9972\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9394    0.9306    0.9350     12500\n",
      "           1     0.9313    0.9399    0.9356     12500\n",
      "\n",
      "   micro avg     0.9353    0.9353    0.9353     25000\n",
      "   macro avg     0.9353    0.9353    0.9353     25000\n",
      "weighted avg     0.9353    0.9353    0.9353     25000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████| 1563/1563 [00:10<00:00, 147.44it/s]\n",
      "Predicting: 100%|█████████████████████████████████████████| 1563/1563 [00:01<00:00, 960.28it/s]\n",
      "Predicting: 100%|█████████████████████████████████████████| 1563/1563 [00:01<00:00, 925.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.01231458126993341\n",
      "Train accuracy:  0.9972\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9393    0.9304    0.9348     12500\n",
      "           1     0.9311    0.9398    0.9354     12500\n",
      "\n",
      "   micro avg     0.9351    0.9351    0.9351     25000\n",
      "   macro avg     0.9352    0.9351    0.9351     25000\n",
      "weighted avg     0.9352    0.9351    0.9351     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(TensorDataset(torch.tensor(cls_vectors, dtype=torch.float),\n",
    "                                      torch.tensor(train.negative_embedding, dtype=torch.float),\n",
    "                                      torch.tensor(train.positive_embedding, dtype=torch.float),\n",
    "                                      torch.tensor(train.polarity)\n",
    "                                     ),\n",
    "                        batch_size=16\n",
    "                       )\n",
    "\n",
    "dataloader_test = DataLoader(TensorDataset(torch.tensor(cls_vectors_test, dtype=torch.float),\n",
    "                                          torch.tensor(test.negative_embedding, dtype=torch.float),\n",
    "                                          torch.tensor(test.positive_embedding, dtype=torch.float),\n",
    "                                          torch.tensor(test.polarity)\n",
    "                                         ),\n",
    "                            batch_size=16\n",
    "                           )\n",
    "\n",
    "\n",
    "bert_llr_model.to('cuda')\n",
    "train_loss = []\n",
    "train_accuracy = []\n",
    "test_f1 = []\n",
    "test_accuracy = []\n",
    "for _ in range(10):\n",
    "    # Training\n",
    "    bert_llr_model.train()\n",
    "    tr_loss = []\n",
    "    for cls_vector, neg_embed, pos_embed, labels in tqdm(dataloader, desc='Training'):\n",
    "        #print(labels)\n",
    "        bert_llr_optimizer.zero_grad()\n",
    "        logits, loss = bert_llr_model(cls_vectors = cls_vector.cuda(), \n",
    "                                      neg_embed = neg_embed.cuda(), \n",
    "                                      pos_embed = pos_embed.cuda(), \n",
    "                                      labels = labels.cuda())\n",
    "        loss.backward()\n",
    "        bert_llr_optimizer.step()\n",
    "        tr_loss.append(loss.item())\n",
    "    loss_score = sum(tr_loss)/len(tr_loss)\n",
    "    train_loss.append(loss_score)\n",
    "    \n",
    "    # Evaluation\n",
    "    bert_llr_model.eval()\n",
    "    predictions = []\n",
    "    for cls_vector, neg_embed, pos_embed, labels in tqdm(dataloader, desc='Predicting'):\n",
    "        with torch.no_grad():\n",
    "            logits = bert_llr_model(cls_vectors = cls_vector.cuda(),\n",
    "                                    neg_embed = neg_embed.cuda(),\n",
    "                                    pos_embed = pos_embed.cuda())[0] #This is for generate predict only\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        predictions.append(logits)\n",
    "    predictions = [j for i in predictions for j in i]\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    acc_score = accuracy_score(train.polarity, predictions)\n",
    "    train_accuracy.append(acc_score)\n",
    "    \n",
    "    # Predict data\n",
    "    predictions = []\n",
    "    for cls_vector, neg_embed, pos_embed, labels in tqdm(dataloader_test, desc='Predicting'):\n",
    "        with torch.no_grad():\n",
    "            logits = bert_llr_model(cls_vectors = cls_vector.cuda(),\n",
    "                                    neg_embed = neg_embed.cuda(),\n",
    "                                    pos_embed = pos_embed.cuda())[0] #This is for generate predict only\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        predictions.append(logits)\n",
    "    preds = [j for i in predictions for j in i]\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    test_f1.append(f1_score(test.polarity, preds, average='macro'))\n",
    "    test_accuracy.append(accuracy_score(test.polarity, preds))\n",
    "    \n",
    "    # Print result\n",
    "    print('Train loss: ', loss_score)\n",
    "    print('Train accuracy: ',acc_score)\n",
    "    print(classification_report(test.polarity, preds, digits=4))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.03237039205795172, 0.015601529313760238, 0.014916918573570952, 0.014337929889264201, 0.01392129101979374, 0.013439818499317859, 0.013197794522653957, 0.012920671569866319, 0.012620894917628358, 0.01231458126993341]\n",
      "[0.99588, 0.99596, 0.99632, 0.99668, 0.99676, 0.9968, 0.99696, 0.99704, 0.9972, 0.9972]\n",
      "[0.9338794049146442, 0.9342391851972003, 0.9343989503832062, 0.9348388939293212, 0.9349988533797737, 0.9351986525467015, 0.9351987906539108, 0.935158721744977, 0.9352786065742882, 0.9351185545452054]\n",
      "[0.93388, 0.93424, 0.9344, 0.93484, 0.935, 0.9352, 0.9352, 0.93516, 0.93528, 0.93512]\n"
     ]
    }
   ],
   "source": [
    "print(train_loss)\n",
    "print(train_accuracy)\n",
    "print(test_f1)\n",
    "print(test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test new method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 11671/11671 [00:59<00:00, 195.54it/s]\n",
      "100%|███████████████████████████████████████████████████| 35604/35604 [03:03<00:00, 194.41it/s]\n"
     ]
    }
   ],
   "source": [
    "label_list = list(label_dict.keys())\n",
    "\n",
    "result = nlp.get_label_term_weighting(FOLDER_PATH+'tmunlp_file.txt', label_list)\n",
    "VECTOR_LEN = 70\n",
    "total_word_dict = {}\n",
    "\n",
    "for label in label_list:\n",
    "    word_dict = nlp.get_keyword(label, result, VECTOR_LEN)\n",
    "    total_word_dict.update(word_dict)\n",
    "    \n",
    "def embedding_1hot(df, words):\n",
    "    arr = []\n",
    "    for i in trange(df.shape[0]):\n",
    "        temp = [words[word] if word in df.sentence[i] else 0 for word in words.keys()]\n",
    "        arr.append(temp)\n",
    "    return arr\n",
    "\n",
    "train['llr_vector'] = embedding_1hot(train, total_word_dict)\n",
    "test['llr_vector'] = embedding_1hot(test, total_word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "560"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(total_word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertLLR(\n",
       "  (bert): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (bert_activation): Tanh()\n",
       "  (llr): Linear(in_features=560, out_features=560, bias=True)\n",
       "  (llr_activation): Tanh()\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=1328, out_features=8, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BertLLR(nn.Module):\n",
    "    def __init__(self, VECTOR_LEN, num_labels=2):\n",
    "        super(BertLLR, self).__init__()\n",
    "        self.bert = nn.Linear(768,768)\n",
    "        self.bert_activation = nn.Tanh()\n",
    "        \n",
    "        self.llr = nn.Linear(len(label_list)*VECTOR_LEN, len(label_list)*VECTOR_LEN)\n",
    "        self.llr_activation = nn.Tanh()\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "        self.classifier = nn.Linear(768+len(label_list)*VECTOR_LEN, num_labels)\n",
    "    \n",
    "    def forward(self, cls_vectors=None, llr_vectors=None, labels=None):\n",
    "        tanh_cls_vectors = self.bert_activation(self.bert(cls_vectors))\n",
    "        tanh_llr_vectors = self.llr_activation(self.llr(llr_vectors))\n",
    "        \n",
    "        concat_vectors = torch.cat([tanh_cls_vectors, tanh_llr_vectors], dim=1)\n",
    "        concat_vectors = self.dropout(concat_vectors)\n",
    "        \n",
    "        logits = self.classifier(concat_vectors)\n",
    "        \n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, 8), labels.view(-1))\n",
    "            outputs = (logits, loss)\n",
    "        else:\n",
    "            outputs = (logits,)\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "bert_llr_model = BertLLR(VECTOR_LEN, num_labels=bert_model.model.config.num_labels)\n",
    "bert_llr_optimizer = torch.optim.Adam(params = bert_llr_model.parameters(), \n",
    "                                      lr = 1e-5\n",
    "                                     )\n",
    "\n",
    "\n",
    "bert_llr_model   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████████████████| 730/730 [00:04<00:00, 134.00it/s]\n",
      "Predicting: 100%|██████████████████████████████████████████| 730/730 [00:00<00:00, 1110.72it/s]\n",
      "Predicting: 100%|████████████████████████████████████████| 2226/2226 [00:01<00:00, 1164.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.6860183621922584\n",
      "Train accuracy:  0.929397652300574\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5596    0.5772    0.5683      4326\n",
      "           1     0.3584    0.9104    0.5143      1473\n",
      "           2     0.4874    0.9803    0.6510      1573\n",
      "           3     0.6234    0.5285    0.5720      7344\n",
      "           4     0.8665    0.6030    0.7111     18266\n",
      "           5     0.4063    0.9240    0.5645      1526\n",
      "           6     0.6146    0.9473    0.7455       835\n",
      "           7     0.3567    0.7395    0.4813       261\n",
      "\n",
      "   micro avg     0.6367    0.6367    0.6367     35604\n",
      "   macro avg     0.5341    0.7763    0.6010     35604\n",
      "weighted avg     0.7119    0.6367    0.6471     35604\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████████████████| 730/730 [00:04<00:00, 168.91it/s]\n",
      "Predicting: 100%|██████████████████████████████████████████| 730/730 [00:00<00:00, 1082.48it/s]\n",
      "Predicting: 100%|████████████████████████████████████████| 2226/2226 [00:01<00:00, 1130.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.25122400152764907\n",
      "Train accuracy:  0.9392511352926056\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5711    0.5608    0.5659      4326\n",
      "           1     0.3370    0.9253    0.4940      1473\n",
      "           2     0.5051    0.9784    0.6662      1573\n",
      "           3     0.6120    0.5327    0.5696      7344\n",
      "           4     0.8721    0.5818    0.6980     18266\n",
      "           5     0.4036    0.9318    0.5633      1526\n",
      "           6     0.5762    0.9557    0.7189       835\n",
      "           7     0.2969    0.8851    0.4447       261\n",
      "\n",
      "   micro avg     0.6269    0.6269    0.6269     35604\n",
      "   macro avg     0.5218    0.7940    0.5901     35604\n",
      "weighted avg     0.7123    0.6269    0.6385     35604\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████████████████| 730/730 [00:04<00:00, 154.91it/s]\n",
      "Predicting: 100%|██████████████████████████████████████████| 730/730 [00:00<00:00, 1060.25it/s]\n",
      "Predicting: 100%|████████████████████████████████████████| 2226/2226 [00:02<00:00, 1063.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.2046105886873317\n",
      "Train accuracy:  0.944049353097421\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5763    0.5564    0.5662      4326\n",
      "           1     0.3275    0.9321    0.4846      1473\n",
      "           2     0.5101    0.9790    0.6707      1573\n",
      "           3     0.6102    0.5344    0.5698      7344\n",
      "           4     0.8755    0.5727    0.6925     18266\n",
      "           5     0.4027    0.9384    0.5636      1526\n",
      "           6     0.5597    0.9605    0.7072       835\n",
      "           7     0.2790    0.9042    0.4264       261\n",
      "\n",
      "   micro avg     0.6229    0.6229    0.6229     35604\n",
      "   macro avg     0.5176    0.7972    0.5851     35604\n",
      "weighted avg     0.7136    0.6229    0.6351     35604\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████████████████| 730/730 [00:04<00:00, 164.17it/s]\n",
      "Predicting: 100%|██████████████████████████████████████████| 730/730 [00:00<00:00, 1173.13it/s]\n",
      "Predicting: 100%|████████████████████████████████████████| 2226/2226 [00:01<00:00, 1131.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.184876923895862\n",
      "Train accuracy:  0.948761888441436\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5809    0.5534    0.5668      4326\n",
      "           1     0.3229    0.9362    0.4802      1473\n",
      "           2     0.5130    0.9797    0.6734      1573\n",
      "           3     0.6103    0.5362    0.5708      7344\n",
      "           4     0.8766    0.5682    0.6895     18266\n",
      "           5     0.4025    0.9450    0.5645      1526\n",
      "           6     0.5499    0.9641    0.7003       835\n",
      "           7     0.2762    0.9195    0.4248       261\n",
      "\n",
      "   micro avg     0.6212    0.6212    0.6212     35604\n",
      "   macro avg     0.5165    0.8003    0.5838     35604\n",
      "weighted avg     0.7144    0.6212    0.6337     35604\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████████████████| 730/730 [00:04<00:00, 158.68it/s]\n",
      "Predicting: 100%|██████████████████████████████████████████| 730/730 [00:00<00:00, 1144.07it/s]\n",
      "Predicting: 100%|████████████████████████████████████████| 2226/2226 [00:02<00:00, 1090.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.17203001229934498\n",
      "Train accuracy:  0.951846457030246\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5807    0.5525    0.5662      4326\n",
      "           1     0.3210    0.9382    0.4784      1473\n",
      "           2     0.5150    0.9803    0.6753      1573\n",
      "           3     0.6086    0.5366    0.5704      7344\n",
      "           4     0.8775    0.5651    0.6875     18266\n",
      "           5     0.4037    0.9502    0.5666      1526\n",
      "           6     0.5439    0.9653    0.6957       835\n",
      "           7     0.2771    0.9310    0.4271       261\n",
      "\n",
      "   micro avg     0.6200    0.6200    0.6200     35604\n",
      "   macro avg     0.5159    0.8024    0.5834     35604\n",
      "weighted avg     0.7144    0.6200    0.6325     35604\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████████████████| 730/730 [00:04<00:00, 150.31it/s]\n",
      "Predicting: 100%|██████████████████████████████████████████| 730/730 [00:00<00:00, 1111.45it/s]\n",
      "Predicting: 100%|████████████████████████████████████████| 2226/2226 [00:02<00:00, 1079.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.16206110637788088\n",
      "Train accuracy:  0.9548453431582555\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5808    0.5550    0.5676      4326\n",
      "           1     0.3181    0.9430    0.4758      1473\n",
      "           2     0.5162    0.9828    0.6769      1573\n",
      "           3     0.6095    0.5342    0.5694      7344\n",
      "           4     0.8771    0.5620    0.6851     18266\n",
      "           5     0.4048    0.9522    0.5681      1526\n",
      "           6     0.5373    0.9665    0.6906       835\n",
      "           7     0.2779    0.9349    0.4284       261\n",
      "\n",
      "   micro avg     0.6187    0.6187    0.6187     35604\n",
      "   macro avg     0.5152    0.8038    0.5827     35604\n",
      "weighted avg     0.7143    0.6187    0.6312     35604\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████████████████| 730/730 [00:04<00:00, 155.56it/s]\n",
      "Predicting: 100%|██████████████████████████████████████████| 730/730 [00:00<00:00, 1145.55it/s]\n",
      "Predicting: 100%|████████████████████████████████████████| 2226/2226 [00:02<00:00, 1059.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.15470511343023957\n",
      "Train accuracy:  0.958101276668666\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5819    0.5550    0.5681      4326\n",
      "           1     0.3200    0.9470    0.4784      1473\n",
      "           2     0.5165    0.9835    0.6773      1573\n",
      "           3     0.6079    0.5357    0.5695      7344\n",
      "           4     0.8780    0.5605    0.6842     18266\n",
      "           5     0.4043    0.9541    0.5680      1526\n",
      "           6     0.5347    0.9677    0.6888       835\n",
      "           7     0.2776    0.9349    0.4281       261\n",
      "\n",
      "   micro avg     0.6186    0.6186    0.6186     35604\n",
      "   macro avg     0.5151    0.8048    0.5828     35604\n",
      "weighted avg     0.7145    0.6186    0.6309     35604\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████████████████| 730/730 [00:03<00:00, 187.67it/s]\n",
      "Predicting: 100%|██████████████████████████████████████████| 730/730 [00:00<00:00, 1142.86it/s]\n",
      "Predicting: 100%|████████████████████████████████████████| 2226/2226 [00:02<00:00, 1077.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.1471286101241226\n",
      "Train accuracy:  0.9595578785022706\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5818    0.5525    0.5668      4326\n",
      "           1     0.3193    0.9491    0.4779      1473\n",
      "           2     0.5162    0.9841    0.6772      1573\n",
      "           3     0.6073    0.5357    0.5692      7344\n",
      "           4     0.8778    0.5586    0.6827     18266\n",
      "           5     0.4031    0.9554    0.5670      1526\n",
      "           6     0.5315    0.9689    0.6865       835\n",
      "           7     0.2776    0.9349    0.4281       261\n",
      "\n",
      "   micro avg     0.6175    0.6175    0.6175     35604\n",
      "   macro avg     0.5143    0.8049    0.5819     35604\n",
      "weighted avg     0.7141    0.6175    0.6298     35604\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████████████████| 730/730 [00:05<00:00, 142.78it/s]\n",
      "Predicting: 100%|██████████████████████████████████████████| 730/730 [00:00<00:00, 1082.75it/s]\n",
      "Predicting: 100%|████████████████████████████████████████| 2226/2226 [00:01<00:00, 1149.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.14172933775013033\n",
      "Train accuracy:  0.9612715277182761\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5815    0.5525    0.5666      4326\n",
      "           1     0.3186    0.9518    0.4774      1473\n",
      "           2     0.5153    0.9847    0.6766      1573\n",
      "           3     0.6077    0.5343    0.5687      7344\n",
      "           4     0.8775    0.5575    0.6818     18266\n",
      "           5     0.4039    0.9574    0.5682      1526\n",
      "           6     0.5288    0.9689    0.6841       835\n",
      "           7     0.2773    0.9349    0.4277       261\n",
      "\n",
      "   micro avg     0.6168    0.6168    0.6168     35604\n",
      "   macro avg     0.5138    0.8052    0.5814     35604\n",
      "weighted avg     0.7139    0.6168    0.6291     35604\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████████████████| 730/730 [00:05<00:00, 144.24it/s]\n",
      "Predicting: 100%|██████████████████████████████████████████| 730/730 [00:00<00:00, 1128.35it/s]\n",
      "Predicting: 100%|████████████████████████████████████████| 2226/2226 [00:02<00:00, 1038.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.1361000793462951\n",
      "Train accuracy:  0.9625567646302802\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5837    0.5532    0.5680      4326\n",
      "           1     0.3189    0.9538    0.4780      1473\n",
      "           2     0.5151    0.9847    0.6764      1573\n",
      "           3     0.6067    0.5349    0.5685      7344\n",
      "           4     0.8769    0.5555    0.6802     18266\n",
      "           5     0.4031    0.9587    0.5676      1526\n",
      "           6     0.5277    0.9701    0.6835       835\n",
      "           7     0.2778    0.9387    0.4287       261\n",
      "\n",
      "   micro avg     0.6162    0.6162    0.6162     35604\n",
      "   macro avg     0.5137    0.8062    0.5814     35604\n",
      "weighted avg     0.7136    0.6162    0.6284     35604\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(TensorDataset(torch.tensor(cls_vectors, dtype=torch.float),\n",
    "                                      torch.tensor(train.llr_vector, dtype=torch.float),\n",
    "                                      torch.tensor(train.polarity)\n",
    "                                     ),\n",
    "                        batch_size=16\n",
    "                       )\n",
    "\n",
    "dataloader_test = DataLoader(TensorDataset(torch.tensor(cls_vectors_test, dtype=torch.float),\n",
    "                                          torch.tensor(test.llr_vector, dtype=torch.float),\n",
    "                                          torch.tensor(test.polarity)\n",
    "                                         ),\n",
    "                            batch_size=16\n",
    "                           )\n",
    "\n",
    "\n",
    "bert_llr_model.to('cuda')\n",
    "train_loss = []\n",
    "train_accuracy = []\n",
    "test_f1 = []\n",
    "test_accuracy = []\n",
    "for _ in range(10):\n",
    "    # Training\n",
    "    bert_llr_model.train()\n",
    "    tr_loss = []\n",
    "    for cls_vector, llr_vector, labels in tqdm(dataloader, desc='Training'):\n",
    "        #print(labels)\n",
    "        bert_llr_optimizer.zero_grad()\n",
    "        logits, loss = bert_llr_model(cls_vectors=cls_vector.cuda(), \n",
    "                                      llr_vectors=llr_vector.cuda(), \n",
    "                                      labels=labels.cuda())\n",
    "        loss.backward()\n",
    "        bert_llr_optimizer.step()\n",
    "        tr_loss.append(loss.item())\n",
    "    loss_score = sum(tr_loss)/len(tr_loss)\n",
    "    train_loss.append(loss_score)\n",
    "    \n",
    "    # Evaluation\n",
    "    bert_llr_model.eval()\n",
    "    predictions = []\n",
    "    for cls_vector, llr_vector, labels in tqdm(dataloader, desc='Predicting'):\n",
    "        with torch.no_grad():\n",
    "            logits = bert_llr_model(cls_vectors=cls_vector.cuda(),\n",
    "                                    llr_vectors=llr_vector.cuda())[0] #This is for generate predict only\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        predictions.append(logits)\n",
    "    predictions = [j for i in predictions for j in i]\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    acc_score = accuracy_score(train.polarity, predictions)\n",
    "    train_accuracy.append(acc_score)\n",
    "    \n",
    "    # Predict data\n",
    "    predictions = []\n",
    "    for cls_vector, llr_vector, labels in tqdm(dataloader_test, desc='Predicting'):\n",
    "        with torch.no_grad():\n",
    "            logits = bert_llr_model(cls_vectors=cls_vector.cuda(),\n",
    "                                    llr_vectors=llr_vector.cuda())[0] #This is for generate predict only\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        predictions.append(logits)\n",
    "    preds = [j for i in predictions for j in i]\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    test_f1.append(f1_score(test.polarity, preds, average='macro'))\n",
    "    test_accuracy.append(accuracy_score(test.polarity, preds))\n",
    "    \n",
    "    # Print result\n",
    "    print('Train loss: ', loss_score)\n",
    "    print('Train accuracy: ',acc_score)\n",
    "    print(classification_report(test.polarity, preds, digits=4))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6860183621922584, 0.25122400152764907, 0.2046105886873317, 0.184876923895862, 0.17203001229934498, 0.16206110637788088, 0.15470511343023957, 0.1471286101241226, 0.14172933775013033, 0.1361000793462951]\n",
      "[0.929397652300574, 0.9392511352926056, 0.944049353097421, 0.948761888441436, 0.951846457030246, 0.9548453431582555, 0.958101276668666, 0.9595578785022706, 0.9612715277182761, 0.9625567646302802]\n",
      "[0.6010029888971391, 0.5900777530863088, 0.5851235517798916, 0.5837785103884101, 0.5833905851589674, 0.5827412881051215, 0.5828136081460616, 0.5819087260351077, 0.5813863297486113, 0.58136705646002]\n",
      "[0.6367262105381418, 0.6268677676665543, 0.6228513650151668, 0.6211942478373217, 0.6200426918323784, 0.6187226154364678, 0.6185821817773284, 0.617458712504213, 0.6168408044039996, 0.6162228963037861]\n"
     ]
    }
   ],
   "source": [
    "print(train_loss)\n",
    "print(train_accuracy)\n",
    "print(test_f1)\n",
    "print(test_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
