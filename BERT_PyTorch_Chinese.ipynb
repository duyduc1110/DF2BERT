{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GeForce RTX 2080 Ti'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "import inspect\n",
    "import time\n",
    "import logging\n",
    "import tmunlp as nlp\n",
    "import random\n",
    "\n",
    "from tqdm import trange, tqdm, tqdm_notebook, tqdm_pandas, tqdm_gui\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from transformers import BertConfig, BertModel, BertTokenizer, BertForSequenceClassification, AdamW, BertPreTrainedModel\n",
    "from transformers import get_constant_schedule_with_warmup\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "from torch import nn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load data & pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data:\n",
      "   polarity                  sentence\n",
      "0         3             王室內鬥 沙國公主求英庇護\n",
      "1         3              嬌娃出國工作 先學防身術\n",
      "2         3        三星手機竟拍到鬼？ 港用戶嚇到找牧師\n",
      "3         7  冰上雅姿盛典（2） (圖) 安麗ARTISTRY\n",
      "4         7                手機會寫『女王英文』\n",
      "\n",
      "Test data:\n",
      "   polarity          sentence\n",
      "0         7    台七甲四季路段坍方 交通中斷\n",
      "1         3    陸客游上大膽島被緝獲 遭羈押\n",
      "2         7    助學生就業 勞委會邀校長座談\n",
      "3         7       創源基因檢測 技術領先\n",
      "4         4  劉以豪遊沖繩「花椰菜」出水變香菇\n"
     ]
    }
   ],
   "source": [
    "def preprocessing(df):\n",
    "    df.polarity = [label_dict[i] for i in df.polarity]\n",
    "    return df\n",
    "\n",
    "def shuffle(df):\n",
    "    index = [i for i in range(df.shape[0])]\n",
    "    random.shuffle(index)\n",
    "    df = df.set_index([index]).sort_index()\n",
    "    return df\n",
    "\n",
    "'''\n",
    "train = pd.read_csv('./dataset/ppt_movie/PPT_Movie_Review_train.txt', sep='\\t', names=['polarity','sentence'])\n",
    "test = pd.read_csv('./dataset/ppt_movie/PPT_Movie_Review_test.txt', sep='\\t', names=['polarity','sentence'])\n",
    "\n",
    "train = shuffle(train)\n",
    "test = shuffle(test)\n",
    "\n",
    "label_list = list(set(train.polarity))\n",
    "label_dict = {}\n",
    "for i in range(len(label_list)):\n",
    "    label_dict[label_list[i]] = i\n",
    "\n",
    "train = preprocessing(train)\n",
    "test = preprocessing(test)\n",
    "'''\n",
    "\n",
    "train = pd.read_csv('./dataset/reader_emotion/train.txt', sep='\\t', index_col=False, names=['polarity','sentence'], header=None)\n",
    "test = pd.read_csv('./dataset/reader_emotion/test.txt', sep='\\t', index_col=False, names=['polarity','sentence'], header=None)\n",
    "\n",
    "train = shuffle(train)\n",
    "test = shuffle(test)\n",
    "\n",
    "label_list = list(set(train.polarity))\n",
    "label_dict = {}\n",
    "for i in range(len(label_list)):\n",
    "    label_dict[label_list[i]] = i\n",
    "\n",
    "train = preprocessing(train)\n",
    "test = preprocessing(test)\n",
    "\n",
    "print('Train data:\\n{}\\n\\nTest data:\\n{}'.format(train.head(5), test.head(5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'王室內鬥 沙國公主求英庇護'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.sentence[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"directionality\": \"bidi\",\n",
       "  \"finetuning_task\": null,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"is_decoder\": false,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"num_labels\": 2,\n",
       "  \"output_attentions\": false,\n",
       "  \"output_hidden_states\": true,\n",
       "  \"output_past\": true,\n",
       "  \"pooler_fc_size\": 768,\n",
       "  \"pooler_num_attention_heads\": 12,\n",
       "  \"pooler_num_fc_layers\": 3,\n",
       "  \"pooler_size_per_head\": 128,\n",
       "  \"pooler_type\": \"first_token_transform\",\n",
       "  \"pruned_heads\": {},\n",
       "  \"torchscript\": false,\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_bfloat16\": false,\n",
       "  \"vocab_size\": 21128\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BertModelBonz():\n",
    "    def __init__(self, model='bert-base-uncased', batch_size=6):\n",
    "        self.pre_trained_model = model\n",
    "        self.config = BertConfig.from_pretrained(self.pre_trained_model, output_hidden_states=True)\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(self.pre_trained_model)\n",
    "        \n",
    "        self.model = BertForSequenceClassification.from_pretrained(self.pre_trained_model, config=self.config)\n",
    "        self.max_len = self.model.config.max_position_embeddings\n",
    "        self.optimizer = AdamW(params = self.model.parameters(), lr=1e-5)\n",
    "        \n",
    "        self.train_loss = []\n",
    "        self.train_accuracy = []\n",
    "        self.test_f1 = []\n",
    "    \n",
    "    def create_ids(self, sentences):\n",
    "        logging.getLogger(\"transformers.tokenization_utils\").setLevel(logging.ERROR) #Disable tokenizer logs, it's really annoy\n",
    "        input_ids = []\n",
    "        for sen in tqdm(sentences, desc=\"Create Ids\"):\n",
    "            tmp = self.tokenizer.encode(sen)\n",
    "            input_ids.append(tmp)\n",
    "        input_ids = pad_sequences(input_ids, \n",
    "                                  maxlen=self.max_len, \n",
    "                                  dtype='int64', \n",
    "                                  truncating='post', \n",
    "                                  padding='post')\n",
    "        return input_ids\n",
    "    \n",
    "    def prepare_data(self, input_ids, input_labels=None):\n",
    "        input_ids = torch.tensor(self.create_ids(input_ids))\n",
    "        if input_labels is None:\n",
    "            return DataLoader(TensorDataset(input_ids), \n",
    "                              batch_size=self.batch_size)\n",
    "        else:\n",
    "            input_labels = torch.tensor(input_labels)\n",
    "            return DataLoader(TensorDataset(input_ids, input_labels), \n",
    "                              batch_size=self.batch_size)\n",
    "        \n",
    "    def flat_accuracy(self, preds, labels):\n",
    "        pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "        labels_flat = labels.flatten()\n",
    "        return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "    \n",
    "    def train(self, train_dataloader, test_dataloader, epochs=4):\n",
    "        self.model.to(device)\n",
    "        for i in trange(epochs, desc=\"Epoch\"):\n",
    "            # Training model\n",
    "            self.model.train()\n",
    "            tr_loss = []\n",
    "            \n",
    "            for input_ids, input_labels in tqdm_notebook(train_dataloader, desc='Training'):\n",
    "                self.optimizer.zero_grad()\n",
    "                loss = self.model(input_ids=input_ids.cuda(), labels=input_labels.cuda())[0] \n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                tr_loss.append(loss.item())\n",
    "            \n",
    "            loss_score = sum(tr_loss)/len(tr_loss)\n",
    "            self.train_loss.append(loss_score)\n",
    "\n",
    "            # Evaluation\n",
    "            self.model.eval()\n",
    "            predictions = []\n",
    "            \n",
    "            for input_ids, input_labels in tqdm_notebook(train_dataloader, desc='Evaluating'):\n",
    "                with torch.no_grad():\n",
    "                    logits = self.model(input_ids=input_ids.cuda())[0]\n",
    "                logits = logits.detach().cpu().numpy()\n",
    "                predictions.append(logits)\n",
    "            \n",
    "            predictions = [j for i in predictions for j in i]\n",
    "            predictions = np.argmax(predictions, axis=1)\n",
    "            acc_score = accuracy_score(predictions, train.polarity)\n",
    "            self.train_accuracy.append(acc_score)\n",
    "            \n",
    "            # Predict test data\n",
    "            self.model.eval()\n",
    "            predictions = []\n",
    "            \n",
    "            for input_ids, input_labels in tqdm_notebook(test_dataloader, desc=\"Predicting\"):\n",
    "                with torch.no_grad():\n",
    "                    logits = self.model(input_ids=input_ids.cuda())[0] #This is for generate predict only\n",
    "                logits = logits.detach().cpu().numpy()\n",
    "                predictions.append(logits)\n",
    "            \n",
    "            predictions = [j for i in predictions for j in i]\n",
    "            predictions = np.argmax(predictions, axis=1)\n",
    "            \n",
    "            self.test_f1.append(f1_score(predictions, test.polarity, average='macro'))\n",
    "            \n",
    "            # Print result\n",
    "            print('EPOCH', i)\n",
    "            print('Train loss: ', loss_score)\n",
    "            print('Train accuracy: ',acc_score)\n",
    "            print(classification_report(predictions, test.polarity, digits=4))\n",
    "            \n",
    "            \n",
    "    def generate_cls_vectors(self, dataloader):\n",
    "        self.model.to('cuda')\n",
    "        self.model.eval()\n",
    "        cls_vectors = []\n",
    "        for input_ids, input_labels in tqdm_notebook(dataloader):\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(input_ids.cuda())\n",
    "                last_hidden_layer = outputs[1][12]\n",
    "                cls_vector = last_hidden_layer[:,0,:]\n",
    "            cls_vector = cls_vector.detach().cpu().numpy()\n",
    "            cls_vectors.extend(cls_vector)\n",
    "        return cls_vectors\n",
    "                \n",
    "   \n",
    "        \n",
    "        \n",
    "#Create model\n",
    "config = BertConfig(output_hidden_states=True)\n",
    "bert_model = BertModelBonz(model='bert-base-chinese', batch_size=6)\n",
    "bert_model.model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Create train & test dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Create Ids: 100%|██████████████████████████████████████████████████████████████| 11671/11671 [00:02<00:00, 5104.88it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many dimensions 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-a0a41b3b163f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_dataloader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbert_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_labels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolarity\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtest_dataloader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbert_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_labels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolarity\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-ab4a08abf826>\u001b[0m in \u001b[0;36mprepare_data\u001b[1;34m(self, input_ids, input_labels)\u001b[0m\n\u001b[0;32m     34\u001b[0m                               batch_size=self.batch_size)\n\u001b[0;32m     35\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m             \u001b[0minput_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m             return DataLoader(TensorDataset(input_ids, input_labels), \n\u001b[0;32m     38\u001b[0m                               batch_size=self.batch_size)\n",
      "\u001b[1;31mValueError\u001b[0m: too many dimensions 'str'"
     ]
    }
   ],
   "source": [
    "train_dataloader = bert_model.prepare_data(input_ids=train.sentence, input_labels=train.polarity)\n",
    "test_dataloader = bert_model.prepare_data(input_ids=test.sentence, input_labels=test.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         feel_depressing\n",
       "1              feel_happy\n",
       "2              feel_happy\n",
       "3                feel_odd\n",
       "4             feel_boring\n",
       "5              feel_angry\n",
       "6             feel_boring\n",
       "7         feel_depressing\n",
       "8             feel_boring\n",
       "9               feel_warm\n",
       "10       feel_informative\n",
       "11       feel_informative\n",
       "12              feel_warm\n",
       "13             feel_happy\n",
       "14       feel_informative\n",
       "15             feel_happy\n",
       "16        feel_depressing\n",
       "17        feel_depressing\n",
       "18            feel_boring\n",
       "19             feel_happy\n",
       "20            feel_boring\n",
       "21        feel_depressing\n",
       "22               feel_odd\n",
       "23            feel_boring\n",
       "24       feel_informative\n",
       "25             feel_angry\n",
       "26               feel_odd\n",
       "27              feel_warm\n",
       "28               feel_odd\n",
       "29        feel_depressing\n",
       "               ...       \n",
       "11641     feel_depressing\n",
       "11642     feel_depressing\n",
       "11643          feel_angry\n",
       "11644            feel_odd\n",
       "11645    feel_informative\n",
       "11646         feel_boring\n",
       "11647            feel_odd\n",
       "11648            feel_odd\n",
       "11649    feel_informative\n",
       "11650            feel_odd\n",
       "11651          feel_happy\n",
       "11652    feel_informative\n",
       "11653            feel_odd\n",
       "11654            feel_odd\n",
       "11655          feel_angry\n",
       "11656    feel_informative\n",
       "11657         feel_boring\n",
       "11658         feel_boring\n",
       "11659         feel_boring\n",
       "11660            feel_odd\n",
       "11661          feel_angry\n",
       "11662            feel_odd\n",
       "11663         feel_boring\n",
       "11664    feel_informative\n",
       "11665          feel_happy\n",
       "11666         feel_boring\n",
       "11667            feel_odd\n",
       "11668         feel_boring\n",
       "11669    feel_informative\n",
       "11670    feel_informative\n",
       "Name: polarity, Length: 11671, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.polarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Train model & Predict data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   0%|                                                                                     | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b069dc8c94924d79aa7e33232fcba844",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training', max=378, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91da2db0701a446890d94a8874adb246",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=378, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45895be8947140cabfd183f83cf640d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Predicting', max=378, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 0\n",
      "Train loss:  0.49981227885714913\n",
      "Train accuracy:  0.8984098939929329\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9037    0.8331    0.8669      1228\n",
      "           1     0.8189    0.8948    0.8552      1036\n",
      "\n",
      "   micro avg     0.8613    0.8613    0.8613      2264\n",
      "   macro avg     0.8613    0.8639    0.8611      2264\n",
      "weighted avg     0.8649    0.8613    0.8616      2264\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  33%|█████████████████████████▎                                                  | 1/3 [03:25<06:51, 205.75s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "518482054416447a8b0a47def2e3e84d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training', max=378, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7db802f8d32a4b5fb3f74bd7a1c49dac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=378, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c1a6a8bcc3a4e35aa5feb7d4fd651c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Predicting', max=378, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 1\n",
      "Train loss:  0.3001104017729482\n",
      "Train accuracy:  0.9147526501766784\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9461    0.7772    0.8534      1378\n",
      "           1     0.7288    0.9312    0.8176       886\n",
      "\n",
      "   micro avg     0.8375    0.8375    0.8375      2264\n",
      "   macro avg     0.8375    0.8542    0.8355      2264\n",
      "weighted avg     0.8611    0.8375    0.8394      2264\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  67%|██████████████████████████████████████████████████▋                         | 2/3 [06:53<03:26, 206.32s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdd92e3892f440dc8c00de53e1dc65b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training', max=378, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c24232ef1c9547aaac8c61cd2e1d4fab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=378, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "526b4185cba64ddb8dd36c2c85101130",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Predicting', max=378, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 2\n",
      "Train loss:  0.20834113496587311\n",
      "Train accuracy:  0.9699646643109541\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8242    0.9023    0.8615      1034\n",
      "           1     0.9108    0.8382    0.8730      1230\n",
      "\n",
      "   micro avg     0.8675    0.8675    0.8675      2264\n",
      "   macro avg     0.8675    0.8703    0.8672      2264\n",
      "weighted avg     0.8712    0.8675    0.8677      2264\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|████████████████████████████████████████████████████████████████████████████| 3/3 [10:16<00:00, 205.34s/it]\n"
     ]
    }
   ],
   "source": [
    "#Train model\n",
    "\n",
    "bert_model.train(train_dataloader, test_dataloader, epochs=3)\n",
    "#torch.save(bert_model.model, 'bert_eb1024_1e5_e4.pth')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.49981227885714913, 0.3001104017729482, 0.20834113496587311] \n",
      " [0.8984098939929329, 0.9147526501766784, 0.9699646643109541] \n",
      " [0.8610576021014448, 0.8355138415614055, 0.8672424186273429]\n"
     ]
    }
   ],
   "source": [
    "print(bert_model.train_loss, '\\n', bert_model.train_accuracy, '\\n', bert_model.test_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(bert_model.model.state_dict(), './dataset/ppt_movie/bert512_epoch2.sd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Generate CLS vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c745ba927934766bca44cbc6b00eb02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=378), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82f82a49d3654317a31d279e80b6017e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=378), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load state dict from trained model\n",
    "filepath = './dataset/ppt_movie/bert512_epoch3.sd'\n",
    "bert_model.model.load_state_dict(torch.load(filepath))\n",
    "\n",
    "cls_vectors = bert_model.generate_cls_vectors(train_dataloader)\n",
    "cls_vectors_test = bert_model.generate_cls_vectors(test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Feature Extraction with TMUNLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Build negative & positive vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "label_list = ['N', 'P']\n",
    "\n",
    "result = nlp.get_label_term_weighting('./dataset/ppt_movie/PPT_Movie_Review_train.txt', label_list)\n",
    "VECTOR_LEN = 70\n",
    "\n",
    "#Create negative word list\n",
    "negative_list =  nlp.get_keyword('N', result, VECTOR_LEN)\n",
    "temp_max = negative_list[list(negative_list.keys())[0]]\n",
    "for i in negative_list.keys():\n",
    "    negative_list[i] = negative_list[i] #/ temp_max\n",
    "\n",
    "\n",
    "#Create postive word list\n",
    "positive_list =  nlp.get_keyword('P', result, VECTOR_LEN)\n",
    "temp_max = positive_list[list(positive_list.keys())[0]]\n",
    "for i in positive_list.keys():\n",
    "    positive_list[i] = positive_list[i] #/ temp_max\n",
    "\n",
    "\n",
    "\n",
    "def embedding_1hot(df, words):\n",
    "    arr = []\n",
    "    for i in range(df.shape[0]):\n",
    "        temp = [words[word] if word in df.sentence[i] else 0 for word in words.keys()]\n",
    "        arr.append(temp)\n",
    "    return arr\n",
    "\n",
    "train['negative_embedding'] = embedding_1hot(train, negative_list)\n",
    "train['positive_embedding'] = embedding_1hot(train, positive_list)\n",
    "\n",
    "test['negative_embedding'] = embedding_1hot(test, negative_list)\n",
    "test['positive_embedding'] = embedding_1hot(test, positive_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Build bert_llr model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertLLR(\n",
       "  (bert): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (bert_activation): Tanh()\n",
       "  (llr): Linear(in_features=140, out_features=140, bias=True)\n",
       "  (llr_activation): Tanh()\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=908, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BertLLR(nn.Module):\n",
    "    def __init__(self, VECTOR_LEN):\n",
    "        super(BertLLR, self).__init__()\n",
    "        self.bert = nn.Linear(768,768)\n",
    "        self.bert_activation = nn.Tanh()\n",
    "        \n",
    "        self.llr = nn.Linear(2*VECTOR_LEN, 2*VECTOR_LEN)\n",
    "        self.llr_activation = nn.Tanh()\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "        self.classifier = nn.Linear(768+2*VECTOR_LEN, 2)\n",
    "    \n",
    "    def forward(self, cls_vectors=None, neg_embed=None, pos_embed=None, labels=None):\n",
    "        tanh_cls_vectors = self.bert_activation(self.bert(cls_vectors))\n",
    "        \n",
    "        llr_vectors = torch.cat([neg_embed, pos_embed], dim=1)\n",
    "        tanh_llr_vectors = self.llr_activation(self.llr(llr_vectors))\n",
    "        \n",
    "        concat_vectors = torch.cat([tanh_cls_vectors, tanh_llr_vectors], dim=1)\n",
    "        concat_vectors = self.dropout(concat_vectors)\n",
    "        \n",
    "        logits = self.classifier(concat_vectors)\n",
    "        \n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, 2), labels.view(-1))\n",
    "            outputs = (logits, loss)\n",
    "        else:\n",
    "            outputs = (logits,)\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "bert_llr_model = BertLLR(VECTOR_LEN)\n",
    "bert_llr_optimizer = torch.optim.Adam(params = bert_llr_model.parameters(), \n",
    "                                      lr = 1e-5\n",
    "                                     )\n",
    "\n",
    "\n",
    "bert_llr_model   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Train bert_llr model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████████████████████████████████████████| 142/142 [00:00<00:00, 146.61it/s]\n",
      "Predicting: 100%|███████████████████████████████████████████████████████████████████| 142/142 [00:00<00:00, 870.78it/s]\n",
      "Predicting: 100%|███████████████████████████████████████████████████████████████████| 142/142 [00:00<00:00, 908.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.21019583087886723\n",
      "Train accuracy:  0.9818904593639576\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8876    0.8304    0.8581      1132\n",
      "           1     0.8407    0.8949    0.8669      1132\n",
      "\n",
      "   micro avg     0.8626    0.8626    0.8626      2264\n",
      "   macro avg     0.8641    0.8626    0.8625      2264\n",
      "weighted avg     0.8641    0.8626    0.8625      2264\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████████████████████████████████████████| 142/142 [00:00<00:00, 179.55it/s]\n",
      "Predicting: 100%|███████████████████████████████████████████████████████████████████| 142/142 [00:00<00:00, 823.00it/s]\n",
      "Predicting: 100%|███████████████████████████████████████████████████████████████████| 142/142 [00:00<00:00, 942.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.0675651283908478\n",
      "Train accuracy:  0.982773851590106\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8875    0.8366    0.8613      1132\n",
      "           1     0.8454    0.8940    0.8690      1132\n",
      "\n",
      "   micro avg     0.8653    0.8653    0.8653      2264\n",
      "   macro avg     0.8665    0.8653    0.8652      2264\n",
      "weighted avg     0.8665    0.8653    0.8652      2264\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████████████████████████████████████████| 142/142 [00:01<00:00, 128.76it/s]\n",
      "Predicting: 100%|███████████████████████████████████████████████████████████████████| 142/142 [00:00<00:00, 765.47it/s]\n",
      "Predicting: 100%|███████████████████████████████████████████████████████████████████| 142/142 [00:00<00:00, 936.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.0580977393235539\n",
      "Train accuracy:  0.9832155477031802\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8864    0.8410    0.8631      1132\n",
      "           1     0.8487    0.8922    0.8699      1132\n",
      "\n",
      "   micro avg     0.8666    0.8666    0.8666      2264\n",
      "   macro avg     0.8676    0.8666    0.8665      2264\n",
      "weighted avg     0.8676    0.8666    0.8665      2264\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████████████████████████████████████████| 142/142 [00:00<00:00, 188.10it/s]\n",
      "Predicting: 100%|███████████████████████████████████████████████████████████████████| 142/142 [00:00<00:00, 842.49it/s]\n",
      "Predicting: 100%|███████████████████████████████████████████████████████████████████| 142/142 [00:00<00:00, 832.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.05368533658362191\n",
      "Train accuracy:  0.9832155477031802\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8878    0.8454    0.8661      1132\n",
      "           1     0.8524    0.8931    0.8723      1132\n",
      "\n",
      "   micro avg     0.8693    0.8693    0.8693      2264\n",
      "   macro avg     0.8701    0.8693    0.8692      2264\n",
      "weighted avg     0.8701    0.8693    0.8692      2264\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████████████████████████████████████████| 142/142 [00:00<00:00, 142.71it/s]\n",
      "Predicting: 100%|███████████████████████████████████████████████████████████████████| 142/142 [00:00<00:00, 884.07it/s]\n",
      "Predicting: 100%|███████████████████████████████████████████████████████████████████| 142/142 [00:00<00:00, 900.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.05158841544368737\n",
      "Train accuracy:  0.9840989399293286\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8866    0.8498    0.8678      1132\n",
      "           1     0.8558    0.8913    0.8732      1132\n",
      "\n",
      "   micro avg     0.8706    0.8706    0.8706      2264\n",
      "   macro avg     0.8712    0.8706    0.8705      2264\n",
      "weighted avg     0.8712    0.8706    0.8705      2264\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████████████████████████████████████████| 142/142 [00:00<00:00, 176.82it/s]\n",
      "Predicting: 100%|███████████████████████████████████████████████████████████████████| 142/142 [00:00<00:00, 813.81it/s]\n",
      "Predicting: 100%|███████████████████████████████████████████████████████████████████| 142/142 [00:00<00:00, 926.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.05030059365844223\n",
      "Train accuracy:  0.9845406360424028\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8859    0.8507    0.8680      1132\n",
      "           1     0.8564    0.8905    0.8731      1132\n",
      "\n",
      "   micro avg     0.8706    0.8706    0.8706      2264\n",
      "   macro avg     0.8712    0.8706    0.8705      2264\n",
      "weighted avg     0.8712    0.8706    0.8705      2264\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████████████████████████████████████████| 142/142 [00:01<00:00, 131.54it/s]\n",
      "Predicting: 100%|███████████████████████████████████████████████████████████████████| 142/142 [00:00<00:00, 912.68it/s]\n",
      "Predicting: 100%|███████████████████████████████████████████████████████████████████| 142/142 [00:00<00:00, 862.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.048889166190170905\n",
      "Train accuracy:  0.9854240282685512\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8859    0.8507    0.8680      1132\n",
      "           1     0.8564    0.8905    0.8731      1132\n",
      "\n",
      "   micro avg     0.8706    0.8706    0.8706      2264\n",
      "   macro avg     0.8712    0.8706    0.8705      2264\n",
      "weighted avg     0.8712    0.8706    0.8705      2264\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████████████████████████████████████████| 142/142 [00:01<00:00, 136.94it/s]\n",
      "Predicting: 100%|███████████████████████████████████████████████████████████████████| 142/142 [00:00<00:00, 918.57it/s]\n",
      "Predicting: 100%|███████████████████████████████████████████████████████████████████| 142/142 [00:00<00:00, 901.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.047720975058914074\n",
      "Train accuracy:  0.9858657243816255\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8878    0.8525    0.8698      1132\n",
      "           1     0.8581    0.8922    0.8748      1132\n",
      "\n",
      "   micro avg     0.8723    0.8723    0.8723      2264\n",
      "   macro avg     0.8729    0.8723    0.8723      2264\n",
      "weighted avg     0.8729    0.8723    0.8723      2264\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████████████████████████████████████████| 142/142 [00:00<00:00, 142.20it/s]\n",
      "Predicting: 100%|███████████████████████████████████████████████████████████████████| 142/142 [00:00<00:00, 878.83it/s]\n",
      "Predicting: 100%|███████████████████████████████████████████████████████████████████| 142/142 [00:00<00:00, 862.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.04661380270050025\n",
      "Train accuracy:  0.9854240282685512\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8889    0.8551    0.8717      1132\n",
      "           1     0.8604    0.8931    0.8765      1132\n",
      "\n",
      "   micro avg     0.8741    0.8741    0.8741      2264\n",
      "   macro avg     0.8747    0.8741    0.8741      2264\n",
      "weighted avg     0.8747    0.8741    0.8741      2264\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████████████████████████████████████████| 142/142 [00:01<00:00, 134.27it/s]\n",
      "Predicting: 100%|███████████████████████████████████████████████████████████████████| 142/142 [00:00<00:00, 857.66it/s]\n",
      "Predicting: 100%|███████████████████████████████████████████████████████████████████| 142/142 [00:00<00:00, 884.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.04483264062920926\n",
      "Train accuracy:  0.9854240282685512\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8896    0.8542    0.8716      1132\n",
      "           1     0.8598    0.8940    0.8766      1132\n",
      "\n",
      "   micro avg     0.8741    0.8741    0.8741      2264\n",
      "   macro avg     0.8747    0.8741    0.8741      2264\n",
      "weighted avg     0.8747    0.8741    0.8741      2264\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(TensorDataset(torch.tensor(cls_vectors, dtype=torch.float),\n",
    "                                      torch.tensor(train.negative_embedding, dtype=torch.float),\n",
    "                                      torch.tensor(train.positive_embedding, dtype=torch.float),\n",
    "                                      torch.tensor(train.polarity)\n",
    "                                     ),\n",
    "                        batch_size=16\n",
    "                       )\n",
    "\n",
    "dataloader_test = DataLoader(TensorDataset(torch.tensor(cls_vectors_test, dtype=torch.float),\n",
    "                                          torch.tensor(test.negative_embedding, dtype=torch.float),\n",
    "                                          torch.tensor(test.positive_embedding, dtype=torch.float),\n",
    "                                          torch.tensor(test.polarity)\n",
    "                                         ),\n",
    "                            batch_size=16\n",
    "                           )\n",
    "\n",
    "\n",
    "bert_llr_model.to('cuda')\n",
    "train_loss = []\n",
    "train_accuracy = []\n",
    "test_f1 = []\n",
    "for _ in range(10):\n",
    "    # Training\n",
    "    bert_llr_model.train()\n",
    "    tr_loss = []\n",
    "    for cls_vector, neg_embed, pos_embed, labels in tqdm(dataloader, desc='Training'):\n",
    "        #print(labels)\n",
    "        bert_llr_optimizer.zero_grad()\n",
    "        logits, loss = bert_llr_model(cls_vectors = cls_vector.cuda(), \n",
    "                                      neg_embed = neg_embed.cuda(), \n",
    "                                      pos_embed = pos_embed.cuda(), \n",
    "                                      labels = labels.cuda())\n",
    "        loss.backward()\n",
    "        bert_llr_optimizer.step()\n",
    "        tr_loss.append(loss.item())\n",
    "    loss_score = sum(tr_loss)/len(tr_loss)\n",
    "    train_loss.append(loss_score)\n",
    "    \n",
    "    # Evaluation\n",
    "    bert_llr_model.eval()\n",
    "    predictions = []\n",
    "    for cls_vector, neg_embed, pos_embed, labels in tqdm(dataloader, desc='Predicting'):\n",
    "        with torch.no_grad():\n",
    "            logits = bert_llr_model(cls_vectors = cls_vector.cuda(),\n",
    "                                    neg_embed = neg_embed.cuda(),\n",
    "                                    pos_embed = pos_embed.cuda())[0] #This is for generate predict only\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        predictions.append(logits)\n",
    "    predictions = [j for i in predictions for j in i]\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    acc_score = accuracy_score(train.polarity, predictions)\n",
    "    train_accuracy.append(acc_score)\n",
    "    \n",
    "    # Predict data\n",
    "    predictions = []\n",
    "    for cls_vector, neg_embed, pos_embed, labels in tqdm(dataloader_test, desc='Predicting'):\n",
    "        with torch.no_grad():\n",
    "            logits = bert_llr_model(cls_vectors = cls_vector.cuda(),\n",
    "                                    neg_embed = neg_embed.cuda(),\n",
    "                                    pos_embed = pos_embed.cuda())[0] #This is for generate predict only\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        predictions.append(logits)\n",
    "    preds = [j for i in predictions for j in i]\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    test_f1.append(f1_score(test.polarity, preds, average='macro'))\n",
    "    \n",
    "    # Print result\n",
    "    print('Train loss: ', loss_score)\n",
    "    print('Train accuracy: ',acc_score)\n",
    "    print(classification_report(test.polarity, preds, digits=4))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.21019583087886723, 0.0675651283908478, 0.0580977393235539, 0.05368533658362191, 0.05158841544368737, 0.05030059365844223, 0.048889166190170905, 0.047720975058914074, 0.04661380270050025, 0.04483264062920926]\n",
      "[0.9818904593639576, 0.982773851590106, 0.9832155477031802, 0.9832155477031802, 0.9840989399293286, 0.9845406360424028, 0.9854240282685512, 0.9858657243816255, 0.9854240282685512, 0.9854240282685512]\n",
      "[0.8624895442064993, 0.8651715493458813, 0.8665201708909145, 0.8691835292555996, 0.8705272405297408, 0.8705318901233121, 0.8705318901233121, 0.8722993728520039, 0.8740711812823452, 0.8740668555806959]\n",
      "0.8695393224190304\n"
     ]
    }
   ],
   "source": [
    "print(train_loss)\n",
    "print(train_accuracy)\n",
    "print(test_f1)\n",
    "print(sum(test_f1)/len(test_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Predict test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03fc733f685741cda1d408c12d45f593",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Predicting', max=1563, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9366    0.9342    0.9354     12533\n",
      "           1     0.9340    0.9365    0.9352     12467\n",
      "\n",
      "   micro avg     0.9353    0.9353    0.9353     25000\n",
      "   macro avg     0.9353    0.9353    0.9353     25000\n",
      "weighted avg     0.9353    0.9353    0.9353     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(TensorDataset(torch.tensor(cls_vectors_test, dtype=torch.float),\n",
    "                                      torch.tensor(test.negative_embedding, dtype=torch.float),\n",
    "                                      torch.tensor(test.positive_embedding, dtype=torch.float),\n",
    "                                      torch.tensor(test.polarity)\n",
    "                                     ),\n",
    "                        batch_size=16\n",
    "                       )\n",
    "\n",
    "bert_llr_model.to('cuda')\n",
    "bert_llr_model.eval()\n",
    "\n",
    "predictions = []\n",
    "for cls_vector, neg_embed, pos_embed, labels in tqdm_notebook(dataloader, desc='Predicting'):\n",
    "    with torch.no_grad():\n",
    "        logits = bert_llr_model(cls_vectors = cls_vector.cuda(),\n",
    "                                neg_embed = neg_embed.cuda(),\n",
    "                                pos_embed = pos_embed.cuda())[0] #This is for generate predict only\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    predictions.append(logits)\n",
    "\n",
    "\n",
    "preds = [j for i in predictions for j in i]\n",
    "preds = np.argmax(preds, axis=1)\n",
    "print(classification_report(preds, test.polarity, digits=4))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
