{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Welcome To Colaboratory",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3fkQIWxgo2I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow-gpu==2.0.0-alpha0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYBJs68fhcuz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "os.getcwd()\n",
        "path = 'Reader_Emotion_Seg_no_stopword.csv'\n",
        "f = pd.read_csv(path)\n",
        "\n",
        "f.head()\n",
        "\n",
        "f['concate'] = f['title_seg_no_stw']+f['content_seg_no_stw']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMGV3RlBheH1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split \n",
        "from sklearn import metrics\n",
        "from tensorflow.python.keras.preprocessing import sequence\n",
        "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.python.keras.utils import np_utils\n",
        "from tensorflow.python.keras import optimizers\n",
        "from tensorflow.python.keras.layers.merge import concatenate\n",
        "from tensorflow.python.keras.models import Sequential, Model\n",
        "from tensorflow.python.keras.layers import Input\n",
        "from tensorflow.python.keras.layers import Dense, Embedding\n",
        "from tensorflow.python.keras.layers import Convolution1D, Flatten, Dropout, MaxPool1D, GlobalAveragePooling1D\n",
        "from tensorflow.python.keras import initializers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nG8krNoehhe4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def textCNN(texts, labels):\n",
        "  training_text, test_text, training_label, test_label = train_test_split(texts, labels, test_size=0.1)\n",
        "\n",
        "  token = Tokenizer(num_words = 20000)\n",
        "  token.fit_on_texts(training_text)\n",
        "  vocab = token.word_index\n",
        "\n",
        "  x_train_seq = token.texts_to_sequences(training_text)\n",
        "  x_test_seq = token.texts_to_sequences(test_text)\n",
        "  x_train = sequence.pad_sequences(x_train_seq, maxlen = 150)\n",
        "  x_test = sequence.pad_sequences(x_test_seq, maxlen = 150)\n",
        "\n",
        "  y_train = np_utils.to_categorical(training_label)\n",
        "  y_test = np_utils.to_categorical(test_label)\n",
        "\n",
        "  num_labels = 8\n",
        "  main_input = Input(shape=(150),), dtype='float64')\n",
        "    # pre-train embeddings\n",
        "    # embedder = Embedding(len(vocab) + 1, 300, input_length = 20, weights = [embedding_matrix], trainable = False)\n",
        "    # embed = embedder(main_input)\n",
        "\n",
        "  embedder = Embedding(len(vocab)+1, 300, input_length=150)\n",
        "  embed = embedder(main_input)\n",
        "\n",
        "    # filter size, region size\n",
        "  cnn = Convolution1D(2, 2, padding='same', strides = 1, activation='relu')(embed)\n",
        "  cnn = MaxPool1D(pool_size=4)(cnn)\n",
        "  flat = Flatten()(cnn)\n",
        "  drop = Dropout(0.2)(flat)\n",
        "  main_output = Dense(num_labels, activation='softmax')(drop)\n",
        "  model = Model(inputs = main_input, outputs = main_output)\n",
        "  model.summary()\n",
        "    \n",
        "  optmzr = optimizers.Adam(lr=0.001)\n",
        "  model.compile(loss = 'categorical_crossentropy', optimizer=optmzr, metrics = ['accuracy'])\n",
        "  train_history = model.fit(x_train, y_train, batch_size = 128, epochs = 100, \n",
        "                              verbose = 2, validation_data=(x_test, y_test))\n",
        "  pre_probability = model.predict(x_test)\n",
        "  predicted = pre_probability.argmax(axis=-1)\n",
        "\n",
        "  from sklearn import metrics\n",
        "  print(\"Classification report for classifier:\\n%s\\n\"\n",
        "  % ( metrics.classification_report(test_label, predicted)))\n",
        "\n",
        "  import matplotlib.pyplot as plt\n",
        "  plt.plot(train_history.history['accuracy'])\n",
        "  plt.plot(train_history.history['val_accuracy'])\n",
        "  plt.title('Train History')\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['train','validation'], loc = \"upper left\")\n",
        "  plt.show()\n",
        "\n",
        "  plt.plot(train_history.history['loss'])\n",
        "  plt.plot(train_history.history['val_loss'])\n",
        "  plt.title('Train History')\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['train','validation'], loc = \"upper left\")\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xb75HWSGhnNu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rnn(texts, labels):\n",
        "  training_text, test_text, training_label, test_label = train_test_split(texts, labels, test_size=0.1)\n",
        "\n",
        "  token = Tokenizer(num_words = 20000)\n",
        "  token.fit_on_texts(training_text)\n",
        "  print(token.document_count)\n",
        "\n",
        "  x_train_seq = token.texts_to_sequences(training_text)\n",
        "  x_test_seq = token.texts_to_sequences(test_text)\n",
        "  x_train = sequence.pad_sequences(x_train_seq, maxlen = 400)\n",
        "  x_test = sequence.pad_sequences(x_test_seq, maxlen = 400)\n",
        "\n",
        "  y_train = np_utils.to_categorical(training_label)\n",
        "  y_test = np_utils.to_categorical(test_label)\n",
        "\n",
        "  from keras.models import Sequential\n",
        "  from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
        "  from keras.layers.embeddings import Embedding\n",
        "  from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(output_dim=32,input_dim=20000,input_length=300))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(units=256,activation='relu'))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(units=128,activation='relu'))\n",
        "  model.add(Dense(units=8,activation='softmax'))\n",
        "  model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  # checkpoint\n",
        "  # filepath=\"C:/Users/doudi/OneDrive/Documents/TMU-GIDS/Lab/Competition/AI cup 2019/weights.best.hdf5\"\n",
        "  checkpoint= ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "  train_history = model.fit(xtrain, y_train_OneHot, validation_data=(xtest, y_test_OneHot), epochs=100, batch_size=100, callbacks=[checkpoint],\n",
        "                            verbose=1, validation_split=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}