{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = 'C:/Users/doudi/Downloads/'\n",
    "os.chdir(path1)\n",
    "\n",
    "file1 = 'Imdb_Seg_no_stopword.csv'\n",
    "imdb = pd.read_csv(file1)\n",
    "imdb_train = imdb.iloc[0:25000,:]\n",
    "imdb_test = imdb.iloc[25000:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>content</th>\n",
       "      <th>content_seg_no_stw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>This is a very bland and inert production of o...</td>\n",
       "      <td>['This', 'bland', 'inert', 'production', 'one'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>I've seen this film in avant-premiere at Imagi...</td>\n",
       "      <td>['I', \"'ve\", 'seen', 'film', 'avant-premiere',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>REVOLT OF THE ZOMBIES (2 outta 5 stars) No, th...</td>\n",
       "      <td>['REVOLT', 'OF', 'THE', 'ZOMBIES', '2', 'outta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>May contain minor spoilers.&lt;br /&gt;&lt;br /&gt;Dressed...</td>\n",
       "      <td>['May', 'contain', 'minor', 'spoilersDressed',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>(spoilers)&lt;br /&gt;&lt;br /&gt;I shoulda figured. The d...</td>\n",
       "      <td>['spoilersI', 'shoulda', 'figured', 'The', 'dv...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tag                                            content  \\\n",
       "0    0  This is a very bland and inert production of o...   \n",
       "1    1  I've seen this film in avant-premiere at Imagi...   \n",
       "2    0  REVOLT OF THE ZOMBIES (2 outta 5 stars) No, th...   \n",
       "3    1  May contain minor spoilers.<br /><br />Dressed...   \n",
       "4    0  (spoilers)<br /><br />I shoulda figured. The d...   \n",
       "\n",
       "                                  content_seg_no_stw  \n",
       "0  ['This', 'bland', 'inert', 'production', 'one'...  \n",
       "1  ['I', \"'ve\", 'seen', 'film', 'avant-premiere',...  \n",
       "2  ['REVOLT', 'OF', 'THE', 'ZOMBIES', '2', 'outta...  \n",
       "3  ['May', 'contain', 'minor', 'spoilersDressed',...  \n",
       "4  ['spoilersI', 'shoulda', 'figured', 'The', 'dv...  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "file2 = 'PTT_movie_seg.csv'\n",
    "PTT = pd.read_csv(file2)\n",
    "PTT_train = PTT.iloc[0:2264,:]\n",
    "PTT_test = PTT.iloc[2264:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>label</th>\n",
       "      <th>content</th>\n",
       "      <th>content_seg</th>\n",
       "      <th>content_no_stw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2264</th>\n",
       "      <td>P</td>\n",
       "      <td>0</td>\n",
       "      <td>原 發表 於 蒼白 皮膚 的 美感 一部 電影 使人 產生 想 看 兩次 的 衝動 至少 要...</td>\n",
       "      <td>['原', '發表', '於', '蒼白', '皮膚', '的', '美感', '一部', ...</td>\n",
       "      <td>['原', '發表', '蒼白', '皮膚', '美感', '一部', '電影', '使人'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2265</th>\n",
       "      <td>P</td>\n",
       "      <td>0</td>\n",
       "      <td>圖文 好 讀版 知道 毛毛蟲 是 怎麼 變成 蝴蝶 的 嗎 毛毛蟲 在 蛹 裡面 融化 成 ...</td>\n",
       "      <td>['圖文', '好', '讀版', '知道', '毛毛蟲', '是', '怎麼', '變成'...</td>\n",
       "      <td>['圖文', '讀版', '知道', '毛毛蟲', '怎麼', '變成', '蝴蝶', '嗎...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2266</th>\n",
       "      <td>P</td>\n",
       "      <td>0</td>\n",
       "      <td>板上 熱烈 討論 是不是 工讀生 發文 的 一部 戲 本來 當時 看 完 懶得 發文 但 看...</td>\n",
       "      <td>['板上', '熱烈', '討論', '是不是', '工讀生', '發文', '的', '一...</td>\n",
       "      <td>['板上', '熱烈', '討論', '是不是', '工讀生', '發文', '一部', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2267</th>\n",
       "      <td>P</td>\n",
       "      <td>0</td>\n",
       "      <td>有 什麼 計畫 在 看 這片 之前 強烈建議 要 先 看過 第一集 才能 對裡面 的 故事 ...</td>\n",
       "      <td>['有', '什麼', '計畫', '在', '看', '這片', '之前', '強烈建議'...</td>\n",
       "      <td>['計畫', '看', '這片', '之前', '強烈建議', '先', '看過', '第一...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2268</th>\n",
       "      <td>P</td>\n",
       "      <td>0</td>\n",
       "      <td>因 剛剛 自己 朋友 在 問好 麻吉 的 定義 到底 是 什麼 我 說 我們 能當 哥們 但...</td>\n",
       "      <td>['因', '剛剛', '自己', '朋友', '在', '問好', '麻吉', '的', ...</td>\n",
       "      <td>['因', '剛剛', '朋友', '問好', '麻吉', '定義', '到底', '能當'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tag  label                                            content  \\\n",
       "2264   P      0  原 發表 於 蒼白 皮膚 的 美感 一部 電影 使人 產生 想 看 兩次 的 衝動 至少 要...   \n",
       "2265   P      0  圖文 好 讀版 知道 毛毛蟲 是 怎麼 變成 蝴蝶 的 嗎 毛毛蟲 在 蛹 裡面 融化 成 ...   \n",
       "2266   P      0  板上 熱烈 討論 是不是 工讀生 發文 的 一部 戲 本來 當時 看 完 懶得 發文 但 看...   \n",
       "2267   P      0  有 什麼 計畫 在 看 這片 之前 強烈建議 要 先 看過 第一集 才能 對裡面 的 故事 ...   \n",
       "2268   P      0  因 剛剛 自己 朋友 在 問好 麻吉 的 定義 到底 是 什麼 我 說 我們 能當 哥們 但...   \n",
       "\n",
       "                                            content_seg  \\\n",
       "2264  ['原', '發表', '於', '蒼白', '皮膚', '的', '美感', '一部', ...   \n",
       "2265  ['圖文', '好', '讀版', '知道', '毛毛蟲', '是', '怎麼', '變成'...   \n",
       "2266  ['板上', '熱烈', '討論', '是不是', '工讀生', '發文', '的', '一...   \n",
       "2267  ['有', '什麼', '計畫', '在', '看', '這片', '之前', '強烈建議'...   \n",
       "2268  ['因', '剛剛', '自己', '朋友', '在', '問好', '麻吉', '的', ...   \n",
       "\n",
       "                                         content_no_stw  \n",
       "2264  ['原', '發表', '蒼白', '皮膚', '美感', '一部', '電影', '使人'...  \n",
       "2265  ['圖文', '讀版', '知道', '毛毛蟲', '怎麼', '變成', '蝴蝶', '嗎...  \n",
       "2266  ['板上', '熱烈', '討論', '是不是', '工讀生', '發文', '一部', '...  \n",
       "2267  ['計畫', '看', '這片', '之前', '強烈建議', '先', '看過', '第一...  \n",
       "2268  ['因', '剛剛', '朋友', '問好', '麻吉', '定義', '到底', '能當'...  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PTT_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reader Emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "file3 = 'Reader_Emotion_Seg_no_stopword.csv'\n",
    "RE = pd.read_csv(file3)\n",
    "RE['concate'] = RE['title_seg_no_stw'] + RE['content_seg_no_stw']\n",
    "RE_train = RE.iloc[0:11671,:]\n",
    "RE_test = RE.iloc[11671:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>tag_Num</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>title_seg_no_stw</th>\n",
       "      <th>content_seg_no_stw</th>\n",
       "      <th>concate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11671</th>\n",
       "      <td>feel_angry</td>\n",
       "      <td>0</td>\n",
       "      <td>三星“Galaxy3S”正式進軍美洲</td>\n",
       "      <td>三星電子21日表示，在世界最大手機市場__美國和主要市場之一的墨西哥正式推出了新款智能手機“...</td>\n",
       "      <td>['三星', 'Galaxy3S', '正式', '進軍', '美洲']</td>\n",
       "      <td>['三星', '電子', '21日', '世界', '手機', '市場', '美國', '主...</td>\n",
       "      <td>['三星', 'Galaxy3S', '正式', '進軍', '美洲']['三星', '電子...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11672</th>\n",
       "      <td>feel_angry</td>\n",
       "      <td>0</td>\n",
       "      <td>國安密帳劉泰英快閃 低調不談案情</td>\n",
       "      <td>政治中心／綜合報導 台北地院22日上午審理國安密帳案，以被告身分傳訊前總統李登輝及劉泰英，不...</td>\n",
       "      <td>['國安密帳', '劉泰英', '快', '閃', '低調', '談', '案情']</td>\n",
       "      <td>['政治中心', '綜合', '台北', '地院', '22日', '上午', '審理', ...</td>\n",
       "      <td>['國安密帳', '劉泰英', '快', '閃', '低調', '談', '案情']['政治...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11673</th>\n",
       "      <td>feel_angry</td>\n",
       "      <td>0</td>\n",
       "      <td>車站前藉口沒錢回家詐財 男遭起訴</td>\n",
       "      <td>苗栗縣一名年青男子多次利用路人的同情心在台北車站附近詐財。男子上個月初又再次犯案被補，經台北...</td>\n",
       "      <td>['車站', '藉口', '錢', '回家', '詐財', '男', '遭', '起訴']</td>\n",
       "      <td>['苗栗縣', '名', '年青', '男子', '利用', '路人', '同情心', '台...</td>\n",
       "      <td>['車站', '藉口', '錢', '回家', '詐財', '男', '遭', '起訴'][...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11674</th>\n",
       "      <td>feel_angry</td>\n",
       "      <td>0</td>\n",
       "      <td>美「狂牛病」後 韓政府解除加強檢疫措施</td>\n",
       "      <td>美國兩個月前發生「狂牛病」病例，時隔兩個月的明天，南韓政府解除加強檢疫措施。這段期間，南韓開...</td>\n",
       "      <td>['美', '狂牛病', '韓', '政府', '解除', '加強', '檢疫', '措施']</td>\n",
       "      <td>['美國', '發生', '狂牛病', '病例', '時隔', '明天', '南韓', '政...</td>\n",
       "      <td>['美', '狂牛病', '韓', '政府', '解除', '加強', '檢疫', '措施'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11675</th>\n",
       "      <td>feel_angry</td>\n",
       "      <td>0</td>\n",
       "      <td>殺人魔律師結辯：將布列維克釋放或收押</td>\n",
       "      <td>挪威雙重恐怖攻擊元兇布列維克(Anders Behring Breivik)的審判今天(22...</td>\n",
       "      <td>['殺人', '魔', '律師', '結辯', '布列維克', '釋放', '收押']</td>\n",
       "      <td>['挪威', '雙重', '恐怖', '攻擊', '元兇', '布列維克', 'Anders...</td>\n",
       "      <td>['殺人', '魔', '律師', '結辯', '布列維克', '釋放', '收押']['挪...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tag  tag_Num                title  \\\n",
       "11671  feel_angry        0   三星“Galaxy3S”正式進軍美洲   \n",
       "11672  feel_angry        0     國安密帳劉泰英快閃 低調不談案情   \n",
       "11673  feel_angry        0     車站前藉口沒錢回家詐財 男遭起訴   \n",
       "11674  feel_angry        0  美「狂牛病」後 韓政府解除加強檢疫措施   \n",
       "11675  feel_angry        0   殺人魔律師結辯：將布列維克釋放或收押   \n",
       "\n",
       "                                                 content  \\\n",
       "11671  三星電子21日表示，在世界最大手機市場__美國和主要市場之一的墨西哥正式推出了新款智能手機“...   \n",
       "11672  政治中心／綜合報導 台北地院22日上午審理國安密帳案，以被告身分傳訊前總統李登輝及劉泰英，不...   \n",
       "11673  苗栗縣一名年青男子多次利用路人的同情心在台北車站附近詐財。男子上個月初又再次犯案被補，經台北...   \n",
       "11674  美國兩個月前發生「狂牛病」病例，時隔兩個月的明天，南韓政府解除加強檢疫措施。這段期間，南韓開...   \n",
       "11675  挪威雙重恐怖攻擊元兇布列維克(Anders Behring Breivik)的審判今天(22...   \n",
       "\n",
       "                                      title_seg_no_stw  \\\n",
       "11671             ['三星', 'Galaxy3S', '正式', '進軍', '美洲']   \n",
       "11672       ['國安密帳', '劉泰英', '快', '閃', '低調', '談', '案情']   \n",
       "11673    ['車站', '藉口', '錢', '回家', '詐財', '男', '遭', '起訴']   \n",
       "11674  ['美', '狂牛病', '韓', '政府', '解除', '加強', '檢疫', '措施']   \n",
       "11675      ['殺人', '魔', '律師', '結辯', '布列維克', '釋放', '收押']   \n",
       "\n",
       "                                      content_seg_no_stw  \\\n",
       "11671  ['三星', '電子', '21日', '世界', '手機', '市場', '美國', '主...   \n",
       "11672  ['政治中心', '綜合', '台北', '地院', '22日', '上午', '審理', ...   \n",
       "11673  ['苗栗縣', '名', '年青', '男子', '利用', '路人', '同情心', '台...   \n",
       "11674  ['美國', '發生', '狂牛病', '病例', '時隔', '明天', '南韓', '政...   \n",
       "11675  ['挪威', '雙重', '恐怖', '攻擊', '元兇', '布列維克', 'Anders...   \n",
       "\n",
       "                                                 concate  \n",
       "11671  ['三星', 'Galaxy3S', '正式', '進軍', '美洲']['三星', '電子...  \n",
       "11672  ['國安密帳', '劉泰英', '快', '閃', '低調', '談', '案情']['政治...  \n",
       "11673  ['車站', '藉口', '錢', '回家', '詐財', '男', '遭', '起訴'][...  \n",
       "11674  ['美', '狂牛病', '韓', '政府', '解除', '加強', '檢疫', '措施'...  \n",
       "11675  ['殺人', '魔', '律師', '結辯', '布列維克', '釋放', '收押']['挪...  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RE_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dnn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn import metrics\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import np_utils\n",
    "from keras import optimizers\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import Convolution1D, Flatten, Dropout, MaxPool1D, GlobalAveragePooling1D\n",
    "from keras import initializers\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "# from keras.layers.recurrent import SimpleRNN\n",
    "from keras.layers.recurrent import LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binary class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# textCNN\n",
    "def textCNN(training_text, test_text, training_label, test_label):\n",
    "    token = Tokenizer(num_words = 20000)\n",
    "    token.fit_on_texts(training_text)\n",
    "    vocab = token.word_index\n",
    "\n",
    "    x_train_seq = token.texts_to_sequences(training_text)\n",
    "    x_test_seq = token.texts_to_sequences(test_text)\n",
    "    x_train = sequence.pad_sequences(x_train_seq, maxlen = 150)\n",
    "    x_test = sequence.pad_sequences(x_test_seq, maxlen = 150)\n",
    "\n",
    "    y_train = np_utils.to_categorical(training_label)\n",
    "    y_test = np_utils.to_categorical(test_label)\n",
    "\n",
    "    num_labels = 2\n",
    "    main_input = Input(shape=(150,), dtype='float64')\n",
    "    # pre-train embeddings\n",
    "    # embedder = Embedding(len(vocab) + 1, 300, input_length = 20, weights = [embedding_matrix], trainable = False)\n",
    "    # embed = embedder(main_input)\n",
    "\n",
    "    embedder = Embedding(len(vocab)+1, 300, input_length=150)\n",
    "    embed = embedder(main_input)\n",
    "\n",
    "    # filter size, region size\n",
    "    cnn = Convolution1D(2, 2, padding='same', strides = 1, activation='relu')(embed)\n",
    "    cnn = MaxPool1D(pool_size=4)(cnn)\n",
    "    flat = Flatten()(cnn)\n",
    "    drop = Dropout(0.2)(flat)\n",
    "    main_output = Dense(num_labels, activation='sigmoid')(drop)\n",
    "    model = Model(inputs = main_input, outputs = main_output)\n",
    "    model.summary()\n",
    "    \n",
    "    optmzr = optimizers.Adam(lr=0.001)\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer=optmzr, metrics = ['accuracy'])\n",
    "    train_history = model.fit(x_train, y_train, batch_size = 128, epochs = 100, \n",
    "                              verbose = 2, validation_split=0.1)\n",
    "    pre_probability = model.predict(x_test)\n",
    "    predicted = pre_probability.argmax(axis=-1)\n",
    "\n",
    "    from sklearn import metrics\n",
    "    print(\"Classification report for classifier:\\n%s\\n\"\n",
    "          % ( metrics.classification_report(test_label, predicted)))\n",
    "    \n",
    "    print(\"f1_score :\\n%s\\n\" % ( metrics.f1_score(test_label, predicted)))\n",
    "    print(\"acc_score :\\n%s\\n\" % ( metrics.accuracy_score(test_label, predicted)))\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(train_history.history['accuracy'])\n",
    "    plt.plot(train_history.history['val_accuracy'])\n",
    "    plt.title('Train History')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['train','validation'], loc = \"upper left\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(train_history.history['loss'])\n",
    "    plt.plot(train_history.history['val_loss'])\n",
    "    plt.title('Train History')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['train','validation'], loc = \"upper left\")\n",
    "    plt.show()\n",
    "\n",
    "# LSTM\n",
    "def lstm(training_text, test_text, training_label, test_label):\n",
    "    token = Tokenizer(num_words = 4000)\n",
    "    token.fit_on_texts(training_text)\n",
    "    print(token.document_count)\n",
    "\n",
    "    x_train_seq = token.texts_to_sequences(training_text)\n",
    "    x_test_seq = token.texts_to_sequences(test_text)\n",
    "    x_train = sequence.pad_sequences(x_train_seq, maxlen = 400)\n",
    "    x_test = sequence.pad_sequences(x_test_seq, maxlen = 400)\n",
    "\n",
    "    y_train = np_utils.to_categorical(training_label)\n",
    "    y_test = np_utils.to_categorical(test_label)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(output_dim=32,input_dim=4000,input_length=400))\n",
    "    model.add(Dropout(0.35))\n",
    "    model.add(LSTM(units=16))\n",
    "    model.add(Dense(units=256,activation='relu'))\n",
    "    model.add(Dropout(0.35))\n",
    "    model.add(Dense(units=128,activation='relu'))\n",
    "    model.add(Dense(units=2,activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    # checkpoint\n",
    "    # filepath=\"C:/Users/doudi/OneDrive/Documents/TMU-GIDS/Lab/Competition/AI cup 2019/weights.best.hdf5\"\n",
    "    # checkpoint= ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "    train_history = model.fit(x_train, y_train, validation_data=(x_test, y_test), \n",
    "                              epochs=100, batch_size=128, verbose=2, validation_split=0.1)\n",
    "    scores = model.evaluate(x_test, y_test, verbose=2)\n",
    "    print(scores[1])\n",
    "    \n",
    "    pre_probability = model.predict(x_test)\n",
    "    predicted = pre_probability.argmax(axis=-1)\n",
    "\n",
    "    from sklearn import metrics\n",
    "    print(\"Classification report for classifier:\\n%s\\n\"\n",
    "          % ( metrics.classification_report(test_label, predicted)))\n",
    "\n",
    "    print(\"f1_score :\\n%s\\n\" % ( metrics.f1_score(test_label, predicted)))\n",
    "    print(\"acc_score :\\n%s\\n\" % ( metrics.accuracy_score(test_label, predicted)))\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(train_history.history['accuracy'])\n",
    "    plt.plot(train_history.history['val_accuracy'])\n",
    "    plt.title('Train History')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['train','validation'], loc = \"upper left\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(train_history.history['loss'])\n",
    "    plt.plot(train_history.history['val_loss'])\n",
    "    plt.title('Train History')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['train','validation'], loc = \"upper left\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# textCNN\n",
    "def textCNN_M(training_text, test_text, training_label, test_label):\n",
    "    token = Tokenizer(num_words = 20000)\n",
    "    token.fit_on_texts(training_text)\n",
    "    vocab = token.word_index\n",
    "\n",
    "    x_train_seq = token.texts_to_sequences(training_text)\n",
    "    x_test_seq = token.texts_to_sequences(test_text)\n",
    "    x_train = sequence.pad_sequences(x_train_seq, maxlen = 150)\n",
    "    x_test = sequence.pad_sequences(x_test_seq, maxlen = 150)\n",
    "\n",
    "    y_train = np_utils.to_categorical(training_label)\n",
    "    y_test = np_utils.to_categorical(test_label)\n",
    "\n",
    "    num_labels = 8\n",
    "    main_input = Input(shape=(150,), dtype='float64')\n",
    "    # pre-train embeddings\n",
    "    # embedder = Embedding(len(vocab) + 1, 300, input_length = 20, weights = [embedding_matrix], trainable = False)\n",
    "    # embed = embedder(main_input)\n",
    "\n",
    "    embedder = Embedding(len(vocab)+1, 300, input_length=150)\n",
    "    embed = embedder(main_input)\n",
    "\n",
    "    # filter size, region size\n",
    "    cnn = Convolution1D(2, 2, padding='same', strides = 1, activation='relu')(embed)\n",
    "    cnn = MaxPool1D(pool_size=4)(cnn)\n",
    "    flat = Flatten()(cnn)\n",
    "    drop = Dropout(0.5)(flat)\n",
    "    main_output = Dense(num_labels, activation='softmax')(drop)\n",
    "    model = Model(inputs = main_input, outputs = main_output)\n",
    "    model.summary()\n",
    "    \n",
    "    optmzr = optimizers.Adam(lr=0.001)\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer=optmzr, metrics = ['accuracy'])\n",
    "    train_history = model.fit(x_train, y_train, batch_size = 128, epochs = 100, \n",
    "                              verbose = 2, validation_data=(x_test, y_test))\n",
    "    pre_probability = model.predict(x_test)\n",
    "    predicted = pre_probability.argmax(axis=-1)\n",
    "\n",
    "    from sklearn import metrics\n",
    "    print(\"Classification report for classifier:\\n%s\\n\"\n",
    "          % ( metrics.classification_report(test_label, predicted)))\n",
    "    \n",
    "    print(\"f1_score :\\n%s\\n\" % ( metrics.f1_score(test_label, predicted)))\n",
    "    print(\"acc_score :\\n%s\\n\" % ( metrics.accuracy_score(test_label, predicted)))\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(train_history.history['accuracy'])\n",
    "    plt.plot(train_history.history['val_accuracy'])\n",
    "    plt.title('Train History')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['train','validation'], loc = \"upper left\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(train_history.history['loss'])\n",
    "    plt.plot(train_history.history['val_loss'])\n",
    "    plt.title('Train History')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['train','validation'], loc = \"upper left\")\n",
    "    plt.show()\n",
    "# LSTM\n",
    "def lstm_M(training_text, test_text, training_label, test_label):\n",
    "    token = Tokenizer(num_words = 4000)\n",
    "    token.fit_on_texts(training_text)\n",
    "    print(token.document_count)\n",
    "\n",
    "    x_train_seq = token.texts_to_sequences(training_text)\n",
    "    x_test_seq = token.texts_to_sequences(test_text)\n",
    "    x_train = sequence.pad_sequences(x_train_seq, maxlen = 400)\n",
    "    x_test = sequence.pad_sequences(x_test_seq, maxlen = 400)\n",
    "\n",
    "    y_train = np_utils.to_categorical(training_label)\n",
    "    y_test = np_utils.to_categorical(test_label)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(output_dim=32,input_dim=4000,input_length=400))\n",
    "    model.add(Dropout(0.35))\n",
    "    model.add(LSTM(units=16))\n",
    "    model.add(Dense(units=256,activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(units=128,activation='relu'))\n",
    "    model.add(Dense(units=8,activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    # checkpoint\n",
    "    # filepath=\"C:/Users/doudi/OneDrive/Documents/TMU-GIDS/Lab/Competition/AI cup 2019/weights.best.hdf5\"\n",
    "    # checkpoint= ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "    train_history = model.fit(x_train, y_train, validation_data=(x_test, y_test), \n",
    "                              epochs=100, batch_size=128, verbose=1, validation_split=0.2)\n",
    "    scores = model.evaluate(x_test, y_test, verbose=2)\n",
    "    print(scores[1])\n",
    "    \n",
    "    pre_probability = model.predict(x_test)\n",
    "    predicted = pre_probability.argmax(axis=-1)\n",
    "\n",
    "    from sklearn import metrics\n",
    "    print(\"Classification report for classifier:\\n%s\\n\"\n",
    "          % ( metrics.classification_report(test_label, predicted)))\n",
    "    \n",
    "    print(\"f1_score :\\n%s\\n\" % ( metrics.f1_score(test_label, predicted)))\n",
    "    print(\"acc_score :\\n%s\\n\" % ( metrics.accuracy_score(test_label, predicted)))\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(train_history.history['accuracy'])\n",
    "    plt.plot(train_history.history['val_accuracy'])\n",
    "    plt.title('Train History')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['train','validation'], loc = \"upper left\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(train_history.history['loss'])\n",
    "    plt.plot(train_history.history['val_loss'])\n",
    "    plt.title('Train History')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['train','validation'], loc = \"upper left\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 150, 300)          40635600  \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 150, 2)            1202      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 37, 2)             0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 74)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 74)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 150       \n",
      "=================================================================\n",
      "Total params: 40,636,952\n",
      "Trainable params: 40,636,952\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From C:\\Users\\doudi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\doudi\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 22500 samples, validate on 2500 samples\n",
      "Epoch 1/100\n",
      " - 12s - loss: 0.6786 - accuracy: 0.5532 - val_loss: 0.5865 - val_accuracy: 0.7358\n",
      "Epoch 2/100\n",
      " - 9s - loss: 0.3934 - accuracy: 0.8318 - val_loss: 0.3269 - val_accuracy: 0.8588\n",
      "Epoch 3/100\n",
      " - 9s - loss: 0.2238 - accuracy: 0.9125 - val_loss: 0.3106 - val_accuracy: 0.8678\n",
      "Epoch 4/100\n",
      " - 8s - loss: 0.1521 - accuracy: 0.9432 - val_loss: 0.3318 - val_accuracy: 0.8710\n",
      "Epoch 5/100\n",
      " - 8s - loss: 0.1151 - accuracy: 0.9574 - val_loss: 0.3709 - val_accuracy: 0.8694\n",
      "Epoch 6/100\n",
      " - 8s - loss: 0.0933 - accuracy: 0.9656 - val_loss: 0.3948 - val_accuracy: 0.8668\n",
      "Epoch 7/100\n"
     ]
    }
   ],
   "source": [
    "CNN_IMDB = textCNN(imdb_train['content_seg_no_stw'], imdb_test['content_seg_no_stw'], imdb_train['tag'], imdb_test['tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_IMDB = lstm(imdb_train['content_seg_no_stw'], imdb_test['content_seg_no_stw'], imdb_train['tag'], imdb_test['tag']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PTT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_PTT = textCNN(PTT_train['content_no_stw'], PTT_test['content_no_stw'], PTT_train['label'], PTT_test['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_PTT = lstm(PTT_train['content_no_stw'], PTT_test['content_no_stw'], PTT_train['label'], PTT_test['label']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reader Emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_RE = textCNN_M(RE_train['concate'], RE_test['concate'], RE_train['tag_Num'], RE_test['tag_Num'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_RE = lstm_M(RE_train['concate'], RE_test['concate'], RE_train['tag_Num'], RE_test['tag_Num']) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
