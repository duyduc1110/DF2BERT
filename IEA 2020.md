## IEA 2020

#### Introduction

At present, big data analysis technology has rapidly developed. Thanks to the growth of advanced computerization, the breakthrough of data storage and extraction technology and the growing Internet, we are able to handle massive data. According to previous research, most of the unstructured data are currently buried in text data, and they are mainly distributed on the Internet. Today, the online community and news media have become the main field of information acquisition and transmission for modern people. For example, if we want to buy new sneakers, we would collect reviews of products from the online community and weigh our decision based on others suggestions; or if we want to know what movie is good, we can also search for reviews from online communities.

In addition, sentiment analysis research is getting more and more attention with an attempt to obtain trends in public by mining opinions that are subjective statements that reflect people’s sentiments or perceptions about topics (Pang et al., 2002). Thanks to the optimization of natural language processing technology, it helps us to accurately analyze the sentiment and opinions of massive texts. Sentiment analysis is an important area of natural language processing (NLP) research. While previous researches on emotions mainly focused on detecting the emotions that the authors of the documents were expressing, it is worthy of note that the reader-emotions, in some aspects, differ from that of the authors and may be even more complex (Lin et al., 2008; Tang and Chen, 2012) , for example,  a news article with the title “The price of crude oil will rise 0.5% next week” is just objectively reporting an event without any emotion, but it may invoke emotions like angry or worried in its readers. Furthermore, it is possible to get more sponsorship opportunities from the company or manufacturer if the articles describing a certain product are able to promote greater emotional resonance in the readers. Online commerce is becoming more important, and more and more customers are able to review online to determine their purchase.

In this article, we want to use feature selection methods to find words with high emotional weight from social network texts, extract key opinion words, and apply word embedding models to encode text and apply it to the concatenation of  different deep neuro networks methods. The validity and accuracy of different learning methods for discriminating text emotions and opinions are discussed.

##### Related work

2.1 Previous Text mining on Social Media 

Texts are one of the most commonly used media for conveying emotions. Identifying important factors affecting emotions in an article is important for understanding human language. With the rapid development of computer communication applications such as social networking sites and blogs, recent research on sentiment classification has attracted more attention from enterprises (Chen et al., 2010; Purver and Batterysby, 2012). Unlike traditional author-oriented text analysis, readers' research on text sentiment has recently increased. For example (Pang et al., 2002) was the first to use machine learning techniques to train review materials and apply them to the positive and negative sentiment classification of movie reviews. (Mishne, 2005) uses mood as a label to train SVM (Cortes and Vapnik, 1995) classifiers on blog text or sentences, respectively. In his research, emoji were used as answers, and text keywords were considered as features. Wu et al. (2006) proposed a sentence-level emotion recognition method using dialogue as a corpus, in which "happy", "unhappy" or "neutral" was assigned to each sentence as its emotion category. Yang et al. (2006) Classification of musical sentiment using Thayer's model (Thayer, 1989). Each music segment can be divided into four categories.

2.2 Data representations

Data representation methods has always been a very important issue in sentiment analysis. Sparse vector representations of text, the so-called bag-of-words model, has a long history in NLP.  Harris (1954) proposed the "bag-of-words" concept. Bag-of-words representation has gradually evolved into the most popular classification before 2010, and it is even used to represent image content in addition to text exploration (Zang Y et al., 2010). Technically, The bag-of-words model is a method of disassembling a sentence into the number of occurrences of each word in language processing, ignoring the order relationship in the sentence.

2013 was a year of Word Embedding technology innovation. Word Embedding is a language modeling and feature learning method for language processing. Originally proposed by Bengio et al. 2003. Collobert, R., & Weston, J. (2008) has demonstrated the powerful function of Word Embedding preprocessing in their research, and has released the neural network architecture based on many research institutes so far. However, word2vec, created by Mikolov et al. 2013, makes Word Embedding truly flourish. The following year Pennington et al. 2014 introduced a powerful set of pre-processing Embedding, GloVe. Word Embedding is considered one of the few successful applications of unsupervised learning currently. The fact that they don't need expensive annotations may be their main advantage. Instead, they can be derived from unannotated corpora that are already available. The innovation of neural network model application and word embedding has opened up a new research horizon for natural language processing.	

The performance of the word embedding pre-trained model is excellent, but the biggest disadvantage is that the word vector is fixed, which is the problem of semantic elimination. Polysemy is a phenomenon that often occurs in natural language, and it is also a manifestation of language flexibility and efficiency. For example, "By the time we reached the opposite bank, the boat was sinking fast." Bank has two common meanings, and the meaning in the sentence means "river side" instead of "organization about money". However, when Word Embedding encodes the word "bank", it cannot distinguish between the two meanings, because although they appear in different contexts, when they are trained with a language model, the sentences regardless of the context undergo word2vec. Both predict the same word bank, and the same word occupies the parameter space of the same line, which results in that two different context information will be encoded into the same word embedding space.

2.3  Contextual word representations 

2018 is a turning point in natural language processing, and conceptual understanding of expressing words and sentences in a way that captures potential meaning and relationships is rapidly developing. Peter et al. 2018 proposed a new language model ELMo (Embedding from Language Models). ELMo is a two-way LSTM model (Grave et al. 2013). Compared with the static content of word embedding in the past, when ELMo is pre-trained, words have the ability to discern contextual meaning. ELMo has a typical two-stage process. The first stage is to use language models for pre-training. The second stage is to extract the network layer corresponding to the word from the pre-trained network and embed it as a new feature downstream when doing downstream tasks. 

Devlin et al. 2019 published BERT (Bidirectional Encoder Representations from Transformers), which is currently the most effective language training model. The biggest difference from ELMo is that it uses the method of Transformers (Vaswani et al. 2017) instead of LSTM. The training results prove the importance of bidirectional pre-training for language representation.











---





##### Reference



Bengio, Y., Ducharme, R., Vincent, P., & Janvin, C. (2003). A Neural Probabilistic Language Model. The Journal of Machine Learning Research, 3, 1137–1155. 

Devlin, J., Chang, M., Lee, K., & Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. *NAACL-HLT*. 

Collobert, R., & Weston, J. (2008). A unified architecture for natural language processing. Proceedings of the 25th International Conference on Machine Learning – ICML ’08, 20(1), 160–167. 

Graves, A., Jaitly, N.,  & Mohamed, A. (2013). Hybrid speech recognition with Deep Bidirectional LSTM. 2013 IEEE Workshop on Automatic Speech Recognition and Understanding, Olomouc, 2013, pp. 273-278. 

Joachims T (1998) Text categorization with suport vector machines: learning with many relevant features. In: Proceedings of the 10th European conference on machine learning. Chemnitz, Germany, pp 137–142 

Lin, K.H., Yang, C.H., and Chen, H.H. (2008). Emotion Classification of Online News Articles from the Reader’s Perspective. In Proceedings of International Conference on Web Intelligence, 220- 226. 

Mikolov, T., Corrado, G., Chen, K., & Dean, J. (2013). Efficient Estimation of Word Representations in Vector Space. Proceedings of the International Conference on Learning Representations (ICLR 2013), 1–12. 

Mishne, G. (2005). Experiments with mood classification in blog posts. In Proceedings of the 1st Workshop on Stylistic Analysis of Text for Information Access. 

McCallum A, Nigam K (1998) A comparison of event models for naive bayes text classification. In: AAAI workshop on learning for text categorization, Madison, WI 

Nowak E, Jurie F, Triggs B (2006) Sampling strategies for bag-of-features image classification. In: Proceedings of the 9th European conference on computer vision, Graz, Austria, pp 490–503 

Pang, B., Lee, L., and Vaithyanathan, S. (2002). Thumbs up?: sentiment classification using machine learning techniques. In Proceedings of the ACL-02 conference on Empirical methods in natural language processing, 79-86.  

Tang, Y.J. and Chen, H.H. (2012). Mining Sentiment Words from Microblogs for Predicting WriterReader-emotion Transition. In Proceedings of 8th International Conference on Language Resources and Evaluation, 1226-1229.

Vaswani, Ashish, et al. “Attention is all you need.” (2017), *Advances in neural information processing systems*. 2017. 

Wallach, Hanna. (2006). Topic modeling: Beyond bag-of-words. ICML 2006 - Proceedings of the 23rd International Conference on Machine Learning. 2006. 977-984. 10.1145/1143844.1143967. 

Winn J, Criminisi A, Minka T (2005) Object categorization by learned universal visual dictionary. In: Proceedings of the 10th IEEE international conference on computer vision, Beijing, China, pp 1800–1807 

Zellig S. Harris (1954) Distributional Structure, WORD, 10:2-3, 146-162, DOI: 10.1080/00437956.1954.11659520 



