## IEA 2020

#### Introduction

At present, big data analysis technology has rapidly developed. Thanks to the growth of advanced computerization, the breakthrough of data storage and extraction technology and the growing Internet, we are able to handle massive data. According to previous research, most of the unstructured data are currently buried in text data, and they are mainly distributed on the Internet. Today, the online community and news media have become the main field of information acquisition and transmission for modern people. For example, if we want to buy new sneakers, we would collect reviews of products from the online community and weigh our decision based on others suggestions; or if we want to know what movie is good, we can also search for reviews from online communities.

In addition, sentiment analysis research is getting more and more attention with an attempt to obtain trends in public by mining opinions that are subjective statements that reflect people’s sentiments or perceptions about topics (Pang et al., 2002). Thanks to the optimization of natural language processing technology, it helps us to accurately analyze the sentiment and opinions of massive texts. Sentiment analysis is an important area of natural language processing (NLP) research. While previous researches on emotions mainly focused on detecting the emotions that the authors of the documents were expressing, it is worthy of note that the reader-emotions, in some aspects, differ from that of the authors and may be even more complex (Lin et al., 2008; Tang and Chen, 2012) , for example,  a news article with the title “The price of crude oil will rise 0.5% next week” is just objectively reporting an event without any emotion, but it may invoke emotions like angry or worried in its readers. Furthermore, it is possible to get more sponsorship opportunities from the company or manufacturer if the articles describing a certain product are able to promote greater emotional resonance in the readers. Online commerce is becoming more important, and more and more customers are able to review online to determine their purchase.

In this article, we want to use feature selection methods to find words with high emotional weight from social network texts, extract key opinion words, and apply word embedding models to encode text and apply it to the concatenation of  different deep neuro networks methods. The validity and accuracy of different learning methods for discriminating text emotions and opinions are discussed.

##### Related work

2.1 Previous Text mining on Social Media 

Texts are one of the most commonly used media for conveying emotions. Identifying important factors affecting emotions in an article is important for understanding human language. With the rapid development of computer communication applications such as social networking sites and blogs, recent research on sentiment classification has attracted more attention from enterprises (Chen et al., 2010; Purver and Batterysby, 2012). Unlike traditional author-oriented text analysis, readers' research on text sentiment has recently increased. For example (Pang et al., 2002) was the first to use machine learning techniques to train review materials and apply them to the positive and negative sentiment classification of movie reviews. (Mishne, 2005) uses mood as a label to train SVM (Cortes and Vapnik, 1995) classifiers on blog text or sentences, respectively. In his research, emoji were used as answers, and text keywords were considered as features. Wu et al. (2006) proposed a sentence-level emotion recognition method using dialogue as a corpus, in which "happy", "unhappy" or "neutral" was assigned to each sentence as its emotion category. Yang et al. (2006) Classification of musical sentiment using Thayer's model (Thayer, 1989). Each music segment can be divided into four categories.



For different 

Batool (2013) 提到這幾年來，社交媒體的興起改變了網絡，社會化和個性化的總體觀點。 來自社交網絡的數據用於不同目的（例如選舉預測，情感分析，市場營銷，溝通，商務和教育）的用途正在日益增加。情緒分析在近年來不同領域的應用，比如行銷領域，Mostafa (2013). 研究提到的，通過分析推特的推文來追蹤社群對於不同手機通訊公司的品牌的看法與信心。Rambocas (2013) 指出情緒分析可實時有效地評估消費者的意見，它允許從非常大的樣本中進行數據收集和分析，而沒有任何障礙和時間延遲。 通過情緒分析，營銷人員可以實時收集有關態度和觀點的豐富數據，而不會影響可靠性，有效性和可概括性。營銷人員還可以在態度和觀點發生時收集反饋，而不必進行冗長而昂貴的市場研究活動；政治領域方面，Wang(2012) 提出了一種用於實時分析公眾對總統候選人在2012年美國大選中的情緒的系統，該系統在微博服務Twitter上表示。他們指出傳統的政治分析已經難以負荷現今資訊量爆炸的情形，情緒分析的應用，讓人們對政治輿情能更有效地快速且持續掌握；Balahadia (2016) 該研究利用意見挖掘和情感分析來開發教師的績效評估工具。這項研究可能會根據學生使用英語或菲律賓語的正面和負面反饋來幫助確定教職員工的優勢和劣勢。 



Balahadia, F. F., Fernando, M. C. G., & Juanatas, I. C. (2016, May). Teacher's performance evaluation tool using opinion mining with sentiment analysis. In *2016 IEEE Region 10 Symposium (TENSYMP)* (pp. 95-98). IEEE.

Batool, R., Khattak, A. M., Maqbool, J., & Lee, S. (2013, June). Precise tweet classification and sentiment analysis. In *2013 IEEE/ACIS 12th International Conference on Computer and Information Science (ICIS)* (pp. 461-466). IEEE.

Mostafa, M. M. (2013). More than words: Social networks’ text mining for consumer brand sentiments. *Expert Systems with Applications*, *40*(10), 4241-4251.

Rambocas, M., & Gama, J. (2013). *Marketing research: The role of sentiment analysis* (No. 489). Universidade do Porto, Faculdade de Economia do Porto.

Wang, H., Can, D., Kazemzadeh, A., Bar, F., & Narayanan, S. (2012, July). A system for real-time twitter sentiment analysis of 2012 us presidential election cycle. In *Proceedings of the ACL 2012 system demonstrations* (pp. 115-120). Association for Computational Linguistics.



2.2 Data representations

Data representation methods has always been a very important issue in sentiment analysis. Sparse vector representations of text, the so-called bag-of-words model, has a long history in NLP.  Harris (1954) proposed the "bag-of-words" concept. Bag-of-words representation has gradually evolved into the most popular classification before 2010, and it is even used to represent image content in addition to text exploration (Zang Y et al., 2010). Technically, The bag-of-words model is a method of disassembling a sentence into the number of occurrences of each word in language processing, ignoring the order relationship in the sentence.

2013 was a year of Word Embedding technology innovation. Word Embedding is a language modeling and feature learning method for language processing. Originally proposed by Bengio et al. 2003. Collobert, R., & Weston, J. (2008) has demonstrated the powerful function of Word Embedding preprocessing in their research, and has released the neural network architecture based on many research institutes so far. However, word2vec, created by Mikolov et al. 2013, makes Word Embedding truly flourish. The following year Pennington et al. 2014 introduced a powerful set of pre-processing Embedding, GloVe. Word Embedding is considered one of the few successful applications of unsupervised learning currently. The fact that they don't need expensive annotations may be their main advantage. Instead, they can be derived from unannotated corpora that are already available. The innovation of neural network model application and word embedding has opened up a new research horizon for natural language processing.	

The performance of the word embedding pre-trained model is excellent, but the biggest disadvantage is that the word vector is fixed, which is the problem of semantic elimination. Polysemy is a phenomenon that often occurs in natural language, and it is also a manifestation of language flexibility and efficiency. For example, "By the time we reached the opposite bank, the boat was sinking fast." Bank has two common meanings, and the meaning in the sentence means "river side" instead of "organization about money". However, when Word Embedding encodes the word "bank", it cannot distinguish between the two meanings, because although they appear in different contexts, when they are trained with a language model, the sentences regardless of the context undergo word2vec. Both predict the same word bank, and the same word occupies the parameter space of the same line, which results in that two different context information will be encoded into the same word embedding space.

2.3  Contextual word representations 

2018 is a turning point in natural language processing, and conceptual understanding of expressing words and sentences in a way that captures potential meaning and relationships is rapidly developing. Peter et al. 2018 proposed a new language model ELMo (Embedding from Language Models). ELMo is a two-way LSTM model (Grave et al. 2013). Compared with the static content of word embedding in the past, when ELMo is pre-trained, words have the ability to discern contextual meaning. ELMo has a typical two-stage process. The first stage is to use language models for pre-training. The second stage is to extract the network layer corresponding to the word from the pre-trained network and embed it as a new feature downstream when doing downstream tasks. 

Devlin et al. 2019 published BERT (Bidirectional Encoder Representations from Transformers), which is currently the most effective language training model. The biggest difference from ELMo is that it uses the method of Transformers (Vaswani et al. 2017) instead of LSTM. The training results prove the importance of bidirectional pre-training for language representation.











---





##### Reference



Bengio, Y., Ducharme, R., Vincent, P., & Janvin, C. (2003). A Neural Probabilistic Language Model. The Journal of Machine Learning Research, 3, 1137–1155. 

Devlin, J., Chang, M., Lee, K., & Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. *NAACL-HLT*. 

Collobert, R., & Weston, J. (2008). A unified architecture for natural language processing. Proceedings of the 25th International Conference on Machine Learning – ICML ’08, 20(1), 160–167. 

Graves, A., Jaitly, N.,  & Mohamed, A. (2013). Hybrid speech recognition with Deep Bidirectional LSTM. 2013 IEEE Workshop on Automatic Speech Recognition and Understanding, Olomouc, 2013, pp. 273-278. 

Joachims T (1998) Text categorization with suport vector machines: learning with many relevant features. In: Proceedings of the 10th European conference on machine learning. Chemnitz, Germany, pp 137–142 

Lin, K.H., Yang, C.H., and Chen, H.H. (2008). Emotion Classification of Online News Articles from the Reader’s Perspective. In Proceedings of International Conference on Web Intelligence, 220- 226. 

Mikolov, T., Corrado, G., Chen, K., & Dean, J. (2013). Efficient Estimation of Word Representations in Vector Space. Proceedings of the International Conference on Learning Representations (ICLR 2013), 1–12. 

Mishne, G. (2005). Experiments with mood classification in blog posts. In Proceedings of the 1st Workshop on Stylistic Analysis of Text for Information Access. 

McCallum A, Nigam K (1998) A comparison of event models for naive bayes text classification. In: AAAI workshop on learning for text categorization, Madison, WI 

Nowak E, Jurie F, Triggs B (2006) Sampling strategies for bag-of-features image classification. In: Proceedings of the 9th European conference on computer vision, Graz, Austria, pp 490–503 

Pang, B., Lee, L., and Vaithyanathan, S. (2002). Thumbs up?: sentiment classification using machine learning techniques. In Proceedings of the ACL-02 conference on Empirical methods in natural language processing, 79-86.  

Tang, Y.J. and Chen, H.H. (2012). Mining Sentiment Words from Microblogs for Predicting WriterReader-emotion Transition. In Proceedings of 8th International Conference on Language Resources and Evaluation, 1226-1229.

Vaswani, Ashish, et al. “Attention is all you need.” (2017), *Advances in neural information processing systems*. 2017. 

Wallach, Hanna. (2006). Topic modeling: Beyond bag-of-words. ICML 2006 - Proceedings of the 23rd International Conference on Machine Learning. 2006. 977-984. 10.1145/1143844.1143967. 

Winn J, Criminisi A, Minka T (2005) Object categorization by learned universal visual dictionary. In: Proceedings of the 10th IEEE international conference on computer vision, Beijing, China, pp 1800–1807 

Zellig S. Harris (1954) Distributional Structure, WORD, 10:2-3, 146-162, DOI: 10.1080/00437956.1954.11659520 



於此篇論文，我們提出一個新的模型，改善文字嵌入結果，結合特徵選演算法和目前最新的自然語言處理方法，並且。分別用英語資料訓練模型並測試，與用中文資料驗證模型之跨語言有效性，總更應用於三個不同的資料集來進行任務。結果顯示英語評論資料上非常有效，而中文評論資料