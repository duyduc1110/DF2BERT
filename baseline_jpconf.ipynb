{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline classification model (tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_model(X_TR, X_TE, Y_TR, Y_TE):\n",
    "    from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "    from sklearn import svm\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from xgboost import XGBClassifier\n",
    "    from sklearn import metrics\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    \n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    \n",
    "    Models = ['Bernoulli NB','Multinomial NB','Svm (linear)','Logistic Regression',\n",
    "              'Random Forest','kNN','Decision Tree','XG Boost']\n",
    "    function = [BernoulliNB(),MultinomialNB(),svm.SVC(kernel=\"linear\"),LogisticRegression(),\n",
    "              RandomForestClassifier(),KNeighborsClassifier(),DecisionTreeClassifier(),\n",
    "                XGBClassifier()]\n",
    "    perform_f1 = []\n",
    "    perform_acc = []\n",
    "    \n",
    "    for i in tqdm(range(len(function))):\n",
    "        model = function[i]\n",
    "        #performance = cross_val_score(model, tfidf_vectorizer.fit_transform(train), test, cv=10, scoring'accuracy')\n",
    "        \n",
    "        func = str(function[i])\n",
    "        print(\"==== \", func[0:func.index('(')], \" ====\")\n",
    "        #print(\"avg-accuracy: {0}\".format(performance.mean()))\n",
    "        print('\\n')\n",
    "        # perform.append(performance.mean())\n",
    "        \n",
    "        # X_TR, X_TE, Y_TR, Y_TE = train_test_split(tfidf_vectorizer.fit_transform(train), test, test_size = 0.10)\n",
    "        \n",
    "        model.fit(X_TR, Y_TR)\n",
    "        model.score(X_TE, Y_TE)\n",
    "        e = Y_TE\n",
    "        p = model.predict(X_TE)\n",
    "        print(metrics.classification_report(e,p))\n",
    "        perform_f1.append(metrics.f1_score(e,p,average='macro'))\n",
    "        perform_acc.append(metrics.accuracy_score(e,p))\n",
    "        \n",
    "    result_f1_table = pd.DataFrame({\"Models\":Models,\"Result f1 scores\":perform_f1})\n",
    "    result_acc_table = pd.DataFrame({\"Models\":Models,\"Result acc scores\":perform_acc})\n",
    "    return result_f1_table, result_acc_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMDB\n",
    "path1 = 'C:/Users/doudi/Downloads/'\n",
    "os.chdir(path1)\n",
    "\n",
    "file1 = 'Imdb_Seg_no_stopword.csv'\n",
    "imdb = pd.read_csv(file1)\n",
    "imdb_train = imdb.iloc[0:25000,:]\n",
    "imdb_test = imdb.iloc[25000:,:]\n",
    "\n",
    "# PTT\n",
    "file2 = 'PTT_movie_seg.csv'\n",
    "PTT = pd.read_csv(file2)\n",
    "PTT_train = PTT.iloc[0:2264,:]\n",
    "PTT_test = PTT.iloc[2264:,:]\n",
    "\n",
    "# RE\n",
    "file3 = 'Reader_Emotion_Seg_no_stopword.csv'\n",
    "RE = pd.read_csv(file3)\n",
    "RE['concate'] = RE['title'] + RE['content']\n",
    "RE_train = RE.iloc[0:11671,:]\n",
    "RE_test = RE.iloc[11671:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMDB_output = baseline_model(imdb_train['content'], imdb_test['content'], \n",
    "                             imdb_train['tag'], imdb_test['tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PTT_output = baseline_model(PTT_train['content'], PTT_test['content'], \n",
    "                            PTT_train['label'], PTT_test['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RE_output = baseline_model(RE_train['concate'], RE_test['concate'], RE_train['tag_Num'], RE_test['tag_Num'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
